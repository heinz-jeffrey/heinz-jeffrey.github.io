<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Learnability (LIN 629)</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="my.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Learnability (LIN 629)</h1>
</header>
<p>65;6203;1c # Course Information</p>
<p><strong>Course:</strong> TThu 15:00-16:20, Frey Hall 326</p>
<p><strong>Instructor:</strong> <a href="http://jeffreyheinz.net/">Jeffrey Heinz</a>, <a href="mailto:jeffrey.heinz@stonybrook.edu">jeffrey.heinz@stonybrook.edu</a></p>
<p><strong>Office Hours:</strong> M 13:00-14:00, W 11:00-13:00, SBS N237</p>
<h1 id="materials">Materials</h1>
<ul>
<li><a href="materials/learnability-LIN629-22F-Heinz.pdf">Syllabus</a></li>
</ul>
<h1 id="course-log">Course Log</h1>
<h2 id="oct-2022">25 Oct 2022</h2>
<ul>
<li>Nick presented on distributional learning (Clark and Eyraud 2007). The <a href="materials/Nick-presentation.pdf">slides are here</a>.</li>
<li>For Thu Oct 27 read string extension learning <a href="https://aclanthology.org/P10-1092.pdf">Heinz 2010</a> (Han presenting)</li>
<li>For those interested:
<ul>
<li>Here is <a href="materials/Clark2013.pdf">(Clark 2013)</a> which builds on the prior work of the subsitutable language learning algorithm to remove much (all?) of the ambiguity in the derivations. Consequently, syntactic structures are learned. For example, he basically recovers the syntactic structure underlying propositional logic. And here is Clark and Yoshinaka’s <a href="materials/ClarkYoshinaka2016.pdf">2016 review</a> of several extensions to the distributional learning framework since 2007.</li>
<li>Here are <a href="materials/Heinz-MLRegTest-2022.pdf">slides</a> from my talk on investigating the generalization powers of NNs on regular languages at the “All Things Language and Computation” series last Friday.</li>
</ul></li>
</ul>
<h2 id="oct-2022-1">20 Oct 2022</h2>
<ul>
<li>We concluded the handout on <a href="materials/idlimit.pdf">identification in the limit</a>.</li>
<li>We discussed RPNI from <a href="materials/de-la-higuera.pdf">de la Higuera’s slides</a>.</li>
<li>ALERGIA and OSTIA adapt these ideas to learn probability distributions over sigma star and string-to-string functions from positive examples only.</li>
<li><a href="https://github.com/tudelft-cda-lab/FlexFringe">flexfringe</a> implements RPNI and ALERGIA and many variants <a href="https://orbilu.uni.lu/handle/10993/32814">paper</a>.</li>
<li>For more details read de la Higuera (2010) <a href="https://doi.org/10.1017/CBO9781139194655">Grammatical Inference</a>.</li>
</ul>
<h2 id="oct-2022-2">18 Oct 2022</h2>
<ul>
<li>We continued our study of <a href="materials/idlimit.pdf">identification in the limit paradigms</a>.</li>
<li>Reminder that project proposals are to be approved by November 1. Please talk to me if you need help coming up with a good project.</li>
<li>Here is the schedule we developed over the next several weeks.
<ul>
<li>Tue Oct 25: Nick on distributional learning <a href="https://www.jmlr.org/papers/volume8/clark07a/clark07a.pdf">Clark and Eyraud 2007</a></li>
<li>Thu Oct 27: Han on string extension learning <a href="https://aclanthology.org/P10-1092.pdf">Heinz 2010</a></li>
<li>Tue Nov 01: Jack on extracting automata from recurrent neural networks <a href="https://arxiv.org/pdf/1711.09576.pdf">Weiss et al. 2018</a></li>
<li>Thu Nov 3 - Tue Nov 8: Rita, Gillian, Yola
<ul>
<li>Generative linguistics and neural networks at 60 <a href="https://doi.org/10.1353/lan.2019.0009">Pater 2019</a></li>
<li>Response by Dunbar <a href="https://doi.org/10.1353/lan.2019.0013">The implementational mapping problem</a></li>
<li>Response by Rawski and Heinz <a href="https://doi.org/10.1353/lan.2019.0021">No free lunch</a></li>
<li>The <a href="https://muse.jhu.edu/issue/40022">whole issue</a></li>
</ul></li>
<li>Thu Nov 10: Logan on evolution and learnability <a href="materials/NowakKomarovaNiyogi2002.pdf">Komorova et al. 2002</a></li>
<li>Thu Nov 15: Salam on learning classes of DFTs <a href="https://proceedings.mlr.press/v34/jardine14a.html">Jardine et al 2014</a></li>
<li>Thu Nov 17: Kenneth on learning TSL <a href="https://proceedings.mlr.press/v153/lambert21a.html">Lambert 2021</a></li>
<li>Adil, TBD</li>
</ul></li>
</ul>
<h2 id="oct-2022-3">13 Oct 2022</h2>
<ul>
<li>Eric <a href="materials/Sclafani-LLP2.pdf">presented</a> and led discussion on Heinz 2010.</li>
<li>Jeff finished going over the last several slides from his <a href="materials/whatdoeslearningmean.pdf">what does learning mean</a> talk, which emphasized how deterministic finite state machines define a parameterized concept class, and whether the concepts are formal (boolean) languages, stochastic languages, or string-to-string functions – so whether the parameter values are boolean, reals, or strings – the learning of these classes is conducted similarly. Papers exist which make these connections explicit for DFA more generally and k-SL and k-SP classes in particular.</li>
<li>For next Tuesday, review the first 10 pages (up to 4.7) of the notes on identification in the limit. (We have already discussed up to 4.5 on Oct 06).</li>
<li>Also for those who are enrolled and have not yet presented yet, please think about which papers you would be interested in presenting over the next few weeks as I would like to schedule. Feel free to get in touch if you need pointers.</li>
</ul>
<h2 id="oct-2022-4">11 Oct 2022</h2>
<ul>
<li>Fall break</li>
</ul>
<h2 id="oct-2022-5">06 Oct 2022</h2>
<ul>
<li>We began our study of <a href="materials/idlimit.pdf">identification in the limit</a>.</li>
<li>Read <a href="http://jeffreyheinz.net/papers/Heinz-2010-LLP.pdf">Heinz 2010</a> for next Thursday. Eric will present.</li>
</ul>
<h2 id="oct-2022-6">04 Oct 2022</h2>
<ul>
<li>Class canceled.</li>
</ul>
<h2 id="sep-2022">29 Sep 2022</h2>
<ul>
<li>We discussed the rest of Chapter 5 of Valiant 2013.</li>
<li>Jeff presented his talk <a href="materials/whatdoeslearningmean.pdf">what does learning mean</a></li>
</ul>
<h2 id="sep-2022-1">27 Sep 2022</h2>
<ul>
<li>Sarah <a href="materials/Morphology-Language-Acquisition-Slides-Payne.pdf">presents</a> on Lignos and Yang 2016.</li>
<li>Read the rest of Valiant 2013 chapter 5 and write answers to <a href="materials/04-pac-chap05-2.html">these questions</a> and send them to me <a href="mailto:jeffrey.heinz@stonybrook.edu">jeffrey.heinz@stonybrook.edu</a>.</li>
</ul>
<h2 id="sep-2022-2">22 Sep 2022</h2>
<ul>
<li>Class cancelled. Try to check out the learning talks on Friday at <a href="https://www.jeffreyheinz.net/events/WMTRPprogram.html">this workshop</a></li>
</ul>
<h2 id="sep-2022-3">20 Sep 2022</h2>
<ul>
<li>Magda <a href="materials/NoisyExamplesMarkowska.pdf">presented</a> on Angluin and Laird’s 1988 paper “Learning from Noisy Examples.”</li>
</ul>
<h2 id="sep-2022-4">15 Sep 2022</h2>
<ul>
<li>John presents on the VC dimension. (<a href="materials/VC-Dimension-Murzaku.pdf">Slides</a>)</li>
</ul>
<h2 id="sep-2022-5">13 Sep 2022</h2>
<ul>
<li>We went over this <a href="materials/pacproofs.pdf">handout</a> that reviewed the proof that the tightest-fit rectangle algorithm pac-learns the class of axis-aligned rectangles and the proof that the elimination algoritm pac-learns the class of monomials.</li>
<li>For Thursday, read sections 1, 2 and 3 of chapter 3 KV94 on the VC dimension.</li>
</ul>
<h2 id="sep-2022-6">08 Sep 2022</h2>
<ul>
<li>We reviewed the preliminary definition of PAC learnability.</li>
<li>We discussed and explained the PAC learnability of axis-aligned rectangles.</li>
<li>We discussed the modified definition of PAC learnability which takes into account the size of the representation of the concepts.</li>
<li>We established a plan for the next several classes
<ul>
<li>Tue Sep 13: Jeff on PAC learning monomials (1.3 in KV94)</li>
<li>Thu Sep 15: John on the VC dimension (3.1, 3.2, 3.3 in KV94 and theorems 3.3,3.4)</li>
<li>Tue Sep 20: Magda on <a href="materials/AngluinLaird1988.pdf">Learning from Noisy Examples</a>, up to 2.3</li>
<li>Thu Sep 22: Class canceled because of <a href="https://www.jeffreyheinz.net/events/WMTRPprogram.html">this workshop</a> At least see the learning talks from 4:30pm on Fri Sep 23!</li>
<li>Tue Sep 27: Sarah on <a href="materials/LignosYang2016.pdf">Morphology and Language Acquisition</a></li>
</ul></li>
</ul>
<h2 id="sep-2022-7">06 Sep 2022</h2>
<ul>
<li>We went over HW03.</li>
<li>We discussed up to 5.7 in Valiant 2013.</li>
<li>We defined PAC learning formally.</li>
</ul>
<h2 id="sep-2022-8">01 Sep 2022</h2>
<ul>
<li>We finished the handout on enumerability and computability.</li>
<li>We discussed the first part of chapter 5 of Valiant 2013 “Probably Approximately Correct”.</li>
<li>For next time
<ul>
<li>Write answers to <a href="materials/03-pac-chap05-1.html">these questions</a> and send them to me <a href="mailto:jeffrey.heinz@stonybrook.edu">jeffrey.heinz@stonybrook.edu</a>. Show and explain your work.</li>
<li>Read <a href="materials/Valiant2014-Chaps4-5.pdf">up to 5.7</a> of Valiant 2013 “Probably Approximately Correct”</li>
</ul></li>
</ul>
<h2 id="sep-2022-9">01 Sep 2022</h2>
<ul>
<li>We finished the handout on enumerability and computability.</li>
<li>We discussed the first part of chapter 5 of Valiant 2013 “Probably Approximately Correct”.</li>
<li>For next time
<ul>
<li>Write answers to <a href="materials/03-pac-chap05-1.html">these questions</a> and send them to me <a href="mailto:jeffrey.heinz@stonybrook.edu">jeffrey.heinz@stonybrook.edu</a>. Show and explain your work.</li>
<li>Read <a href="materials/Valiant2014-Chaps4-5.pdf">up to 5.7</a> of Valiant 2013 “Probably Approximately Correct”</li>
</ul></li>
</ul>
<h2 id="aug-2022">30 Aug 2022</h2>
<ul>
<li>We discussed chapter 3 of Valiant 2013.</li>
<li>We explained enumerability and why there are <a href="materials/toofewgrammars.pdf">too few grammars</a>.</li>
<li>For next time
<ul>
<li>Read <a href="materials/Valiant2014-Chaps4-5.pdf">chapter 4 and up to 5.5</a> of Valiant 2013 “Probably Approximately Correct”</li>
<li>Here are the <a href="materials/Valiant2014-NotesGlossaryIndex.pdf">footnotes, glossary and index</a> of Valiant 2013 “Probably Approximately Correct”</li>
</ul></li>
</ul>
<h2 id="aug-2022-1">25 Aug 2022</h2>
<ul>
<li>We discussed chapters 1 and 2 of Valiant 2013.</li>
<li>We finished discussing <a href="materials/verorez.pdf">language learning on planet Verorez</a>.</li>
<li>For next time
<ul>
<li>Read chapter 3 of <a href="materials/Valiant2014-Chap3.pdf">Valiant 2013 “Probably Approximately Correct”</a></li>
<li>Write answers to <a href="materials/02-pac-chap-03.html">these questions</a> and send them to me <a href="mailto:jeffrey.heinz@stonybrook.edu">jeffrey.heinz@stonybrook.edu</a>. Remember, conciseness is a virtue.</li>
</ul></li>
</ul>
<h2 id="aug-2022-2">23 Aug 2022</h2>
<ul>
<li>We went over the syllabus.</li>
<li>We studied <a href="materials/verorez.pdf">language learning on planet Verorez</a>.</li>
<li>For next time
<ul>
<li>Read chapters 1 and 2 of <a href="materials/Valiant2014-Chaps1-2.pdf">Valiant 2013 “Probably Approximately Correct”</a></li>
<li>Write answers to <a href="materials/01-pac-chaps-01-02.html">these questions</a> and send them to me <a href="mailto:jeffrey.heinz@stonybrook.edu">jeffrey.heinz@stonybrook.edu</a>. Remember, conciseness is a virtue.</li>
</ul></li>
</ul>
<hr />

<p><i><script language="Javascript">
document.write("Last updated: " + document.lastModified +"");
</SCRIPT></i></p>
</body>
</html>
