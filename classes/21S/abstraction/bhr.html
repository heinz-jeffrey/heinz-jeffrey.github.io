<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>  

<h1>Phonological Abstractness in the Mental Lexicon by Eric Baković, Jeffrey Heinz, Jonathan Rawski</h1>

<h1 id="talking-and-listening">Talking and Listening</h1>
<p>When we try to understand how we speak, and how we listen, an unavoidable fact is that we use the same vocabulary items again and again. So, not only are these stored in memory, but they are learned on a language-specific basis. In this way, we directly confront the issue of mental representations of speech; that is, something about how we store, retrieve and remember units of speech. What are these units? What are these mental representations? These questions are central to generative phonology, a fact reflected in the title of a collection of important papers by Morris Halle, a founder of the field: <em>From Memory to Speech and Back Again</em> <span class="citation">(Halle 2003)</span>.</p>
<p>In this chapter we review some of the most basic — and most fascinating — conclusions and open questions in phonology regarding how abstract these mental representations of speech are. In an era of big data, and data mining, the prevailing attitude in science and scientific observation seems to be to store every last detail, to never assume any detail is irrelevant, no matter how small, because you never know when it may in fact make a difference somewhere. This attitude is also present in the psychology of language, where it has been shown that people are sensitive to astounding details of aspects of speech reflecting people’s age, gender, social status, neighborhood, and health <span class="citation">(Coleman 2002; Pierrehumbert 2002; Johnson 2006)</span>. <span class="citation">Pierrehumbert (2016)</span> reviews this literature and highlights some theoretical accounts (see also Purse et al, this volume).</p>
<p>In the context of this prevailing attitude, the theory of generative phonology makes some surprising claims. First, it claims it is <em>necessary</em> to abstract away from much of this detail to explain systematic aspects of the pronunciations of the morphemes, words, and phrases we utter. In other words, while a person is sensitive to subtle phonetic details which are informative about various aspects of a speaker’s condition, the theory of generative phonology claims those very same details are <em>irrelevant</em> to the very same persons’ mental representations of the <em>pronunciations</em> of morphemes and words. As such the mental representations of these pronunciations are particular abstractions. We review the arguments for this position in section [sec:pml]. Section [sec:phoneme] shows how these ideas lead to the <em>phoneme</em>, a necessarily abstract category of speech units, which is said to be the fundamental unit out of which the pronunciation of vocabulary items are built.</p>
<p>We then turn to a question posed by Paul Kiparsky: <em>how abstract is phonology?</em> <span class="citation">(Kiparsky 1968)</span> — or, more specifically, <em>how abstract are the mental representations of speech?</em> A vigorous back and forth debate over this question followed, but now, over fifty years later, there is still no consensus regarding the answer. A basic problem is that some languages present good evidence that morphemes are specified with phonological content that is never realized as such in any surface manifestation of those morphemes. At issue is precisely what kind of evidence justifies such abstract phonological content. In our minds, this question, and others related to it, are particularly important and remain among the most interesting, and understudied, questions in phonology today.</p>
<p>But first, we begin this chapter with some thoughts on the nature of abstraction and idealization.</p>
<h1 id="sec:what-is">What is abstraction?</h1>
<p>The first thing to point out about abstraction is not how odd it is, but how common it is. Orthographic letters are abstractions. The capital letter “A” and the lowercase letter “a” are quite different, and yet at some level of abstraction they are referring to the same thing. Money is an abstraction. Whole numbers and fractions are also abstractions. For example, there is no such thing as “three.” There are only examples of collections of three items. Such abstractions are not just taken for granted, but they are valued: the earlier toddlers and young children learn such abstractions, the more we marvel at their intelligence.</p>
<p>A very common approach to problem solving one learns in grade school is shown in the diagram in Figure 1. The diagram shows three boxes. The box on the bottom is labeled &quot;complicated messy system&quot;. An arrow labeled &quot;abstraction&quot; goes upward from it to another box. That box is labeled &quot;Problem&quot;. An arrow extends rightward from the &quot;Problem&quot; box to the third box, which is labeled &quot;Solution&quot;. An arrow labeled &quot;Realization&quot; goes downward from this box back to the first one.</p>
<p>For example, in an elementary-level math class, students are often given a problem expressed in plain language. Their job is to extract the relevant information, organize it into an equation, solve the equation, and report back the answer. Figure 2 shows a 4th grade math exercise. This diagram shows three boxes in the same layout as Figure 1. The &quot;messy complicated system&quot; box is divided in two sections, one left and one right. On the left it says &quot;Question: A farmer has two dozen pigs. Seven more pigs are born, and two pass away. How many pigs are there now?&quot; In the problem box, it says &quot;24 + 7 − 2&quot;. The right arrow from the probem box to the solution box is now labeled &quot;solving&quot;. The solution box now contains &quot;= 29&quot;. The realization arrow brings us back to the right section of the first box. There it says &quot;Answer: 29 pigs&quot;.</p>
<p>Similarly, when responding to challenges that his law of falling bodies did not correspond to the real world, Galileo used an analogy to accounting:</p>
<p>what happens in the concrete [<span class="math inline">…</span>] happens in the abstract. It would be novel indeed if computations and ratios made in abstract numbers should not thereafter correspond to concrete gold and silver coins and merchandise [<span class="math inline">…</span>] (quoted in <span class="citation">Wootton (2015, 23–24)</span>)</p>
<p>Here Galileo mentions an important feature: one can make computations with abstract numbers without directly referencing the particular objects that implement them. Given a proper abstraction, one can make an inference or calculation at the level of numbers with results that then correspond to some specific physical effect or process. This property, that a given abstraction can have many implementations, is known as <strong>multiple realizability</strong> <span class="citation">(Bickle 2020)</span>. An abstract object is multiply-realizable by a number of concrete objects. The concrete objects might differ from each other in various ways, but the ways in which they differ are irrelevant to the abstraction. Orthographic letters, and the mental representations of units of speech, are abstract in this way, too.</p>
<p>The second important point about abstraction is that there are degrees of abstraction. Even the question in Figure [fig:example] as written in plain language is abstract since, for example, it uses numbers to explain the situation, and not for instance a photograph of the two dozen pigs and/or video of seven pigs being born and two others dying. Figure [fig:degree] illustrates the concept of degrees of abstraction. The only change we would make to it would be to add question marks after each of the subcaptions in the figure. The question is always “Is the level of abstraction too realistic? too abstract? or just right?”</p>
<p>Figure [fig:degree] is titled Degrees of Abstraction. It is an image taken from <img src="fig3.png" alt="Degrees of Abstraction (from https://computersciencewiki.org/index.php/Abstraction)." style="width:75.0%" /></p>
<p>Figure [fig:degree] is titled &quot;Degrees of Abstraction. It is an image taken from <a href="https://computersciencewiki.org/index.php/Abstraction" class="uri">https://computersciencewiki.org/index.php/Abstraction</a>. In the image the words &quot;Abstract-o-meter&quot; are displayed prominently. Below these words is a measuring tape extending in linearly in two directions. On the far left it says &quot;Too realistic&quot;. Above those words is a bloody human heart pierced with a feathered wooden arrow. In the middle it says &quot;Just right&quot;. Above those words is a platonic red heart symbol with a platonic black arrow through it. On the far right it says &quot;Too abstract&quot;. Above those words is a red square with two black disconnected rectangles centered in the middle of the red square. They are menat to convey that a single black rectangle going through the square.</p>
<p>Answering this question is not easy. It is not at all obvious what the right level of abstraction is. It is appropriate to deliberately consider whether a particular degree of abstraction is appropriate or not. We are in an age of big data and the milieu of the age seems to be to err on the side of “too realistic” and to avoid analyses that are “too abstract.” Our own view is that this is shortsighted. Many things were at one time considered to be “too abstract” but are now recognized as being perfectly reasonable and in fact essential and useful.</p>
<p>When one studies the history of mathematics, it is interesting how notions once considered to be “too abstract” were considered crazy or useless. This includes things like the number 0, real numbers, <span class="math inline">$\sqrt{-1}$</span>, uncountable infinity, number theory, and so on. When Pythagoras and his group realized that <span class="math inline">$\sqrt{2}$</span> could not be expressed as a fraction of whole numbers, they called it “irrational,” literally “unreasonable.” The term sticks today. Cantor went mad after realizing that infinitely-sized sets had different degrees of cardinality. Today this fact underlies the Church-Turing thesis of computability. The development of the complex numbers caused much costernation in the 17th and 18th centuries but are routinely used today to understand complex physical systems. Developments in number theory in the early part of the 20th century had no purpose other than to satisfy some strange mathematical aesthetic, and now underlie secure cryptographic communications, protecting business operations, journalists, and other vital communications.</p>
<p>In short, we are sympathetic to the view put forth by <span class="citation">Cheng (2015, 22)</span>: “Abstraction can appear to take you further and further away from reality, but really you’re getting closer and closer to the heart of the matter”.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>In this chapter we show that abstractness is a property exploited both by speakers of natural languages and by scientists describing the linguistic knowledge of those speakers.</p>
<h1 id="sec:pml">Phonology and the Mental Lexicon</h1>
<p>The pronunciation of a word is a sequence of events in time. From the perspective of speech production, these events are defined articulatorily. From the perspective of speech perception, these events can be defined acoustically and perceptually. The International Phonetic Alphabet (IPA) defines categories of speech sounds articulatorily and provides a symbol for each categorized speech sound. For consonants, these symbols specify three aspects of articulation: the place of articulation, the manner of articulation, and the activity of the vocal folds. Symbols representing vowels primarily specify the degree of jaw aperture, how forward the positioning of the root of the tongue is, and any rounding of the lips. For example, the pronunciation of the word ‘math’ is transcribed in the IPA as [(Begin IPA) M, ash, theta (End IPA)], as there are three distinct speech sounds in sequence: [(Begin IPA) M (End IPA)], which is articulated by stopping airflow at the lips but releasing it through the nose, with the vocal folds held together such that they vibrate; [(Begin IPA) ash (End IPA)], which is an open front unrounded vowel; and [(Begin IPA) theta (End IPA)], which is articulated by constricting but not stopping airflow with the blade of the tongue between the teeth with the vocal folds spread apart. For most speech acts, these articulations are similar across speakers of the same idiolect, despite individual physiological variation. For a word like ‘tune’, one transcription of it using the IPA is [(Begin IPA) T, U, N (End IPA)], often referred to as a <em>broad transcription</em>. In contrast, a <em>narrow transcription</em> of this word using the IPA is [(Begin IPA) T, H, nasalized U, long mark, N (End IPA)]. The difference between the broad and narrow transcriptions is the degree of abstraction. Both transcriptions reveal systematic aspects of standard American English speech. However, the broad transcription only includes so-called <em>constrastive</em> information and the narrow transcription includes some <em>non-contrastive</em> information as well, indicated in this case with various diacritic marks: the aspiration on the , indicated with the super H, and the nasalization with the tilde above the vowel and extra duration of the vowel with [(Begin IPA) long mark (End IPA)]. Both kinds of transcriptions can be used as abstract representations of the pronunciation of this word stored in memory, and one could ask whether the long term memory representation of the pronunciation of the word ‘tune’ is more like the broad transcription, the narrow transcription, or something else. One can ask if there is only one long-term memory representation of the pronunciation of the word ‘tune’, or if there are multiple, possibly partially redundant representations. All of these hypotheses are open to study. In the remainder of this chapter, we use broad IPA transcriptions as we discuss representations of the pronunciations of words. The degree of abstractness chosen is not critical, but we settle here to facilitate additional discussion.</p>
<p>With this in mind, we ask the question: what does modern generative phonological theory say about the mental representations of speech? The central empirical fact that informs this question is that, in many languages of the world, morphemes are pronounced in different ways depending on context. To illustrate with an example, <span class="citation">Odden (2014)</span> draws attention to the pattern exhibited by the different verb forms in Kerewe shown in Table [tab:kerewe].</p>
<hr />
<table>
<caption>Kerewe verbs, from <span class="citation">Odden (2014, 88–89)</span><span data-label="tab:kerewe"></span></caption>
<colgroup>
<col width="18%" />
<col width="17%" />
<col width="17%" />
<col width="16%" />
<col width="29%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Infinitive</th>
<th align="left">1sg habitual</th>
<th align="left">3sg habitual</th>
<th align="left">Imperative</th>
<th align="left">Gloss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Begin IPA) K, U, P, A, A, M, B, A (End IPA)</td>
<td align="left">(Begin IPA) M, P, A, A, M, B, A (End IPA)</td>
<td align="left">(Begin IPA) A, P, A, A, M, B, A (End IPA)</td>
<td align="left">(Begin IPA) P, A, A, M, B, A (End IPA)</td>
<td align="left">‘adorn’</td>
</tr>
<tr class="even">
<td align="left">(Begin IPA) K, U, P, A, A, engma, G, A (End IPA)</td>
<td align="left">(Begin IPA) M, P, A, A, engma, G, A (End IPA)</td>
<td align="left">(Begin IPA) A, P, A, A, engma, G, A (End IPA)</td>
<td align="left">(Begin IPA) P, A, A, engma, G, A (End IPA)</td>
<td align="left">‘line up’</td>
</tr>
<tr class="odd">
<td align="left">(Begin IPA) K, U, P, I, M, A (End IPA)</td>
<td align="left">(Begin IPA) M, P, I, M, A (End IPA)</td>
<td align="left">(Begin IPA) A, P, I, M, A (End IPA)</td>
<td align="left">(Begin IPA) P, I, M, A (End IPA)</td>
<td align="left">‘measure’</td>
</tr>
<tr class="even">
<td align="left">(Begin IPA) K, U, P, U, U, P, A (End IPA)</td>
<td align="left">(Begin IPA) M, P, U, U, P, A (End IPA)</td>
<td align="left">(Begin IPA) A, P, U, U, P, A (End IPA)</td>
<td align="left">(Begin IPA) P, U, U, P, A (End IPA)</td>
<td align="left">‘be light’</td>
</tr>
<tr class="odd">
<td align="left">(Begin IPA) K, U, P, E, K, E, T, esh, A (End IPA)</td>
<td align="left">(Begin IPA) M, P, E, K, E, T, esh, A (End IPA)</td>
<td align="left">(Begin IPA) A, P, E, K, E, T, esh, A (End IPA)</td>
<td align="left">(Begin IPA) P, E, K, E, T, esh, A (End IPA)</td>
<td align="left">‘make fire with stick’</td>
</tr>
<tr class="even">
<td align="left">(Begin IPA) K, U, P, I, I, N, D, A (End IPA)</td>
<td align="left">(Begin IPA) M, P, I, I, N, D, A (End IPA)</td>
<td align="left">(Begin IPA) A, P, I, I, N, D, A (End IPA)</td>
<td align="left">(Begin IPA) P, I, I, N, D, A (End IPA)</td>
<td align="left">‘be bent’</td>
</tr>
<tr class="odd">
<td align="left">(Begin IPA) K, U, H, I, I, G, A (End IPA)</td>
<td align="left">(Begin IPA) M, P, I, I, G, A (End IPA)</td>
<td align="left">(Begin IPA) A, H, I, I, G, A (End IPA)</td>
<td align="left">(Begin IPA) H, I, I, G, A (End IPA)</td>
<td align="left">‘hunt’</td>
</tr>
<tr class="even">
<td align="left">(Begin IPA) K, U, H, E, E, K, A (End IPA)</td>
<td align="left">(Begin IPA) M, P, E, E, K, A (End IPA)</td>
<td align="left">(Begin IPA) A, H, E, E, K, A (End IPA)</td>
<td align="left">(Begin IPA) H, E, E, K, A (End IPA)</td>
<td align="left">‘carry’</td>
</tr>
<tr class="odd">
<td align="left">(Begin IPA) K, U, H, A, A, engma, G, A (End IPA)</td>
<td align="left">(Begin IPA) M, P, A, A, engma, G, A (End IPA)</td>
<td align="left">(Begin IPA) A, H, A, A, engma, G, A (End IPA)</td>
<td align="left">(Begin IPA) H, A, A, engma, G, A (End IPA)</td>
<td align="left">‘create’</td>
</tr>
<tr class="even">
<td align="left">(Begin IPA) K, U, H, E, E, B, A (End IPA)</td>
<td align="left">(Begin IPA) M, P, E, E, B, A (End IPA)</td>
<td align="left">(Begin IPA) A, H, E, E, B, A (End IPA)</td>
<td align="left">(Begin IPA) H, E, E, B, A (End IPA)</td>
<td align="left">‘guide’</td>
</tr>
<tr class="odd">
<td align="left">(Begin IPA) K, U, H, I, I, M, A (End IPA)</td>
<td align="left">(Begin IPA) M, P, I, I, M, A (End IPA)</td>
<td align="left">(Begin IPA) A, H, I, I, M, A (End IPA)</td>
<td align="left">(Begin IPA) H, I, I, M, A (End IPA)</td>
<td align="left">‘gasp’</td>
</tr>
<tr class="even">
<td align="left">(Begin IPA) K, U, H, U, U, H, A (End IPA)</td>
<td align="left">(Begin IPA) M, P, U, U, H, A (End IPA)</td>
<td align="left">(Begin IPA) A, H, U, U, H, A (End IPA)</td>
<td align="left">(Begin IPA) H, U, U, H, A (End IPA)</td>
<td align="left">‘breathe into’</td>
</tr>
</tbody>
</table>
<hr />
<p>There is an interesting difference between the first group of verb forms and the second group of verb forms. In the first group, for example, the pronunciation for the verb stem ‘adorn’ is consistently regardless of whether it is in the infinitival form (prefixed with [(Begin IPA) K, U (End IPA)], the 1sg habitual form (prefixed with [(Begin IPA) M (End IPA)], the 3sg habitual form (prefixed with [(Begin IPA) A (End IPA)], or the imperative form (not prefixed). It thus makes sense to assume that /(Begin IPA) P, A, A, M, B, A (End IPA)/ represents in a Kerewe speaker’s mental lexicon the major features of the pronunciation of the verb stem ‘adorn’.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> However, when the same kind of morphological analysis is applied to the forms of the verb ‘hunt’ in the second group of verb forms, we find that the verb stem’s pronunciation in the 1sg habitual form is [(Begin IPA) P, I, I, G, A (End IPA)] whereas in the other forms it is [(Begin IPA) H, I, I, G, A (End IPA)]. So the question naturally arises, what long-term memory representation of the pronunciation of the verb stem ‘hunt’ do speakers of Kerewe have?</p>
<p>One possibility is that both [(Begin IPA) P, I, I, G, A (End IPA)] and [(Begin IPA) H, I, I, G, A (End IPA)] are stored, along with the knowledge that [(Begin IPA) P, I, I, G, A (End IPA)] is used in the 1sg habitual form and that [(Begin IPA) H, I, I, G, A (End IPA)] is used otherwise. This is fine as far as it goes, but it is of interest that the other verbs in this second group pattern exactly the same way: they begin with [(Begin IPA) P (End IPA)] in the 1sg habitual form, and with [(Begin IPA) H (End IPA)] otherwise. Furthermore, there are no verb stems in Kerewe which begin with in the 1sg habitual form. Taken together, these observations suggest there is something <em>systematic</em> about the variation in the pronunciation of the various forms of this group of verbs in Kerewe, a systematicity that storage of all pronunciations plus information about their distributions does not readily or insightfully capture.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p>The methods of modern generative phonology lead analysts to posit that the long-term memory representation of the pronunciation of the verb stem ‘hunt’ that speakers of Kerewe have is /(Begin IPA) H, I, I, G, A (End IPA)/, and that <em>H</em> representations are transformed to <em>P</em> representations when they immediately follow <em>M</em> representations. Consequently, the 1sg habitual form of ‘hunt’ is [(Begin IPA) M, P, I, I, G, A), ], ], , B, E, C, A, U, S, E, , I, T, , D, E, R, I, V, E, S, , F, R, O, M, , /, (, (, M, H, I, I, G, A (End IPA)/ by application of this phonological transformation.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> This phonological analysis explains the systematic variation observed because it predicts that every H-initial verb stem ought to be realized as P-initial when it follows any prefix ending with <em>M</em>, such as the 1sg habitual.</p>
<p>There are two key reasons why the mental representation of ‘hunt’ cannot instead be /(Begin IPA) P, I, I, G, A (End IPA)/, with <em>P</em> transformed to <em>H</em> when it immediately follows not-<em>M</em>. First, it is assumed that phonological transformations cannot make direct reference to the negation of a class of speech sounds; in this case, to ‘not-<em>M</em>’. Second, and more significantly, it has been independently determined that the members of the first group of verb stems in Table [tab:kerewe] all begin with /(Begin IPA) P (End IPA)/, and yet the putative transformation of <em>P</em> to <em>H</em> immediately following not-<em>M</em> does not apply to those verb forms. Positing that members of the first group of verb stems begin with /(Begin IPA) P (End IPA)/ and that members of the second group begin with /(Begin IPA) H (End IPA)/ (transformed to <em>P</em> immediately following <em>M</em>) succinctly and systematically distinguishes the two groups.</p>
<p>One argument against this position is that there are cases where it does seem like two distinct pronunciations of a morpheme are stored. A well-known example is the nominative suffix in Korean, which has two pronunciations: [(Begin IPA) K, A (End IPA)], which is suffixed onto vowel-final words, and [(Begin IPA) I (End IPA)], which is suffixed onto consonant-final words. This alternation is phonologically conditioned, but it is nevertheless a stretch of a phonologist’s imagination to concoct a rule that transforms /(Begin IPA) K, A (End IPA)/ to [(Begin IPA) I (End IPA)], or /(Begin IPA) I (End IPA)/ to [(Begin IPA) K, A (End IPA)], or some other machination that relates these two forms of the nominative suffix in some phonological way. Furthermore, there are no other places in the Korean lexicon or language which exemplify any K ~ ∅ or I ~ A alternation. In other words, a single posited underlying form and rules (whatever they are) would only account for the alternation between [(Begin IPA) K, A (End IPA)] and [(Begin IPA) I (End IPA)] in the nominative suffix.</p>
<p>So in Korean, the best analysis of the nominative suffix alternation appears to be a long-term memory representation of the pronunciation of the suffix as {/(Begin IPA) I (End IPA)/, /(Begin IPA) K, A (End IPA)/} with the choice of which one to select being based on the phonological properties of the stem the suffix attaches to. Given that such examples exist in the world’s languages, the question is: why don’t we just use the same kind of analysis for e.g. Kerewe? The answer is the one given above. The systematicity observed in the realization of Kerewe verb stems points to the straightforward phonological analysis in that case, and the lack of systematicity observed in the realization of the Korean nominative suffix points to the alternant selection analysis in that case. For more discussion of phonologically conditioned allomorphy, see <span class="citation">Nevins (2011)</span>.</p>
<p>The argument just made is the basic one for the position that morphemes are stored in long-term memory with a single representation, known as the <em>underlying form</em>. The fact that the same morpheme can be pronounced in different ways depending on context is due to phonological transformations of this underlying form into its various <em>surface forms</em>. The fact that these transformations are systematically applied explains the systematicity in the alternations of the morphemes across the vocabulary of the language.</p>
<p>The phonological analysis in the generative tradition thus comes with two symbiotic parts. The first posits, where systematicity and its explanation demand it, a single mental representation in long-term memory of the pronunciation of a lexical item — that item’s underlying form. The second is the transformational part of a phonological grammar which defines how underlying forms are mapped to surface forms, which are more concrete representations of the pronunciation of the lexical item in the particular context in which it is realized. For example, in the case of Kerewe, the underlying form for ‘hunt’ is /(Begin IPA) H, I, I, G, A (End IPA)/, the underlying form for the 1sg habitual form of ‘hunt’ is /(Begin IPA) M, H, I, I, G, A (End IPA)/, there is a phonological transformation changing <em>H</em> to <em>P</em> immediately following <em>M</em>, and so the surface form in this case is [(Begin IPA) M, P, I, I, G, A (End IPA)].</p>
<p>Once underlying forms and transformations making them distinct from some of their corresponding surface forms are posited, a natural question arises:<em>how distinct can underlying forms be from surface forms?</em> This is the question of abstractness in phonology. We approach this question first from the perspective of two interrelated, fundamental concepts in phonological theory, phonemes and distinctive features, in section [sec:phoneme], setting the stage for discussion of more specific examples of evidence for different types of abstractness in analyses of phonological patterns in section [sec:how].</p>
<h1 id="sec:phoneme">Phonemes and Features</h1>
<p>Much of the question of abstractness in phonology concerns the individual units of speech whose concatenation makes up the pronunciation of lexical items. For example, the speech sounds identically transcribed as <em>K</em> in the Kerewe verb stems [(Begin IPA) H, E, E, K, A (End IPA)] ‘carry’ and [(Begin IPA) P, E, K, E, T, tie bar, esh, A (End IPA)] ‘make fire with stick’ are the same at some level of abstraction; they are just being utilized in different contexts in different lexical items. Precise aspects of the articulation or these two instances of ** may well exhibit systematic differences, parallel to the way that the (narrowly transcribed) [(Begin IPA) T, H (End IPA)]] in ‘tick’, the [(Begin IPA) T (End IPA)]] in ‘stick’, and the [(Begin IPA) fishhook (End IPA)] in ‘attic’ differ systematically in English but are also the same, and at the same level of abstraction as the <em>K</em> in Kerewe.</p>
<p>The idea that people’s mental lexicons make use of a mental alphabet of abstract speech sounds has a very long tradition in linguistics, dating back at least to Pāini’s grammar of Sanskrit, likely written in the 6th–5th century BCE.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> Details of individual theories aside, the speech sounds of this abstract mental alphabet are referred to as <em>phonemes</em>. Phonemes are obviously abstractions: they represent groups or categories of speech sounds, abstracting away from many phonetic particulars, such as the systematic and relatively easily perceptible differences between [(Begin IPA) T, H (End IPA)]], [(Begin IPA) T (End IPA)]], and [(Begin IPA) fishhook (End IPA)]] in the English example above. In this section we trace the evidence for the psychological reality of phonemes and highlight two prominent views of phonemes:one as an indivisible alphabetic symbol, and the other as a local confluence of more basic units or properties known as <em>distinctive features</em>.</p>
<p>Readers interested in a more comprehensive review and discussion of the phoneme are directed to <span class="citation">Dresher (2011)</span>, who distinguishes three views of the phoneme as an entity: the phoneme as <em>physical reality</em>, the phoneme as <em>psychological concept</em>, and the phoneme as <em>theoretical fiction</em>. <span class="citation">Dresher (2011, 245)</span> concludes that “once we abandon empiricist assumptions about science and psychology, there is no obstacle to considering the phoneme to be a psychological entity.” This is an important framing, because it means that any linguistic work describing the nature and content of the phoneme is by necessity a statement about mental representation, in much the same way as the concept of underlying forms discussed in section [sec:pml]. So the question becomes, how much abstraction or idealization is involved in such a psychological entity?</p>
<p>Edward Sapir was the first to present evidence for the psychological reality of phonemes. What is perhaps Sapir’s better-known article, “The Psychological Reality of Phonemes” <span class="citation">(Sapir 1933)</span>, was preceded by his article in the first issue of the journal <em>Language</em>, “Sound Patterns of Language” <span class="citation">(Sapir 1925)</span>. Together, these two articles establish the perspective that (1) the phoneme is a psychological unit of speech, and (2) the character of the phoneme requires taking into account how it functions in the larger phonological context of the language; it cannot be understood solely from investigation of the articulatory or acoustic properties of its surface realizations. <span class="citation">Sapir (1925)</span> argued that languages with the same surface inventory of speech sounds could be organized differently at the underlying, phonemic level. <span class="citation">Sapir (1933)</span> argued that the same phoneme-level organization can explain the behavior of native speakers of a language, whether it be with respect to errors they make in the perception or production of speech or in terms of the choices they make when devising or using a novel writing system to represent the speech sounds of their language.</p>
<p>Nearly a century later, psychological evidence for phonemes continues to be found and presented in the literature. The first issue of <em>Language</em> in 2020, 95 years after the publication of <span><span class="citation">Sapir (1925)</span>’s <span class="citation">(1925)</span></span> article in the very first issue of the same journal, includes an article by William Labov arguing that the regularity of a sound change in Philadelphia is understandable to the extent that speakers of this dialect of English have an abstract mental representation of the front unrounded mid vowel <span class="citation">(Labov 2020)</span>.</p>
<p>Jumping back in time to observations by <span class="citation">Bloomfield (1933)</span> and <span class="citation">Bloch (1941)</span>, among others:English unstressed vowels reduce to schwa in many contexts, making schwa an allophone of every English vowel phoneme. Thus we have [(Begin IPA) F, O, fishhook, schwa, G, turned R, ash, F (End IPA)] ‘photograph’, with a primary-stressed [(Begin IPA) O (End IPA)], an unstressed [(Begin IPA) schwa (End IPA)], and a secondary-stressed [(Begin IPA) ash (End IPA)]], alternating with [(Begin IPA) F, schwa, T, H, alpha, G, turned R, schwa, F, I (End IPA)] ‘photography’, with primary-stressed [(Begin IPA) alpha (End IPA)] flanked by unstressed schwas. It follows that phonemic representations of English morphemes must include unreduced vowels only, with reduction to schwa being due to a phonological transformation, and moreover that the underlying form of many morphemes, such as /(Begin IPA) F, O, T, alpha, G, turned R, ash, F (End IPA)/ ‘photograph’, will not directly correspond to <em>any</em> of their complete surface manifestations due to the nature of stress assignment in English.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> Phonemic analysis is the set of methods by which a language’s underlying inventory of phonemes is induced from the distribution and behavior of its surface speech sounds, which can of course differ from language to language. Borrowing an example from <span class="citation">Hayes (2009, 31–34)</span>, English and Spanish have a set of surface speech sounds that can be broadly transcribed as [(Begin IPA) T, , D, , eth, , fishhook (End IPA)],<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> but their distribution and behavior in each language is such that:</p>
<ul>
<li><p>/(Begin IPA) fishhook (End IPA)/is a phoneme distinct from /(Begin IPA) T (End IPA)/ and /(Begin IPA) D (End IPA)/ in Spanish, but not in English, where [(Begin IPA) fishhook (End IPA)] is sometimes a surface realization of /(Begin IPA) T (End IPA)/ (e.g. ‘bat’, ‘batter’) and other times a surface realization of /(Begin IPA) D (End IPA)/(e.g. ‘sad’, ‘sadder’), and</p></li>
<li><p>/(Begin IPA) eth (End IPA)/ is a phoneme distinct from /(Begin IPA) T (End IPA)/ and /(Begin IPA) D (End IPA)/ in English, but not in Spanish, where [(Begin IPA) eth (End IPA)] is always a surface realization of /(Begin IPA) D (End IPA)/ (e.g. [(Begin IPA) U, N, , D, I, S, K, O (End IPA)] ‘a record’, [(Begin IPA) L, O, S, , eth, I, S, K, O, S (End IPA)] ‘the records’).</p></li>
</ul>
<p>From the same surface inventory of speech sounds [(Begin IPA) T, , D, , eth, , fishhook (End IPA)], then, we arrive at a different phonemic inventory in each language: /(Begin IPA) T, , D, , eth (End IPA)/ in English and /(Begin IPA) T, , D, , fishhook (End IPA)/ in Spanish, with a conditioned variant of in English and a conditioned variant of in Spanish.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
<p>These examples illustrate the basis of important development in 20th Century linguistics: the idea that the phoneme is not the minimal unit after all, and that instead there are subphonemic units — <em>distinctive features</em> — out of which phonemes are built. Generally speaking, distinctive features are used to describe phonologically-relevant phonetic distinctions between speech sounds:those phonetic distinctions that are common to similarly-behaving sounds, and those that differ between the conditioned variants of phonemes. In Spanish, for example, the complementary distribution of [(Begin IPA) eth (End IPA)] and [(Begin IPA) D (End IPA)] is matched by complementary distribution of [(Begin IPA) B (End IPA)] and [(Begin IPA) beta (End IPA)] and of [(Begin IPA) G (End IPA)] and [(Begin IPA) gamma (End IPA)], and the members of each of these pairs are related to each other in precisely the same way: [(Begin IPA) B, , D, , G (End IPA)] are <em>voiced stops</em> (vibrating vocal folds and stopping of airflow at three different places of articulation), while [(Begin IPA) beta, , eth, , gamma (End IPA)] are voiced <em>continuants</em> (constriction but not stopping of airflow at corresponding places of articulation). In English, the related speech sounds [(Begin IPA) T, , D, , fishhook (End IPA)] are all <em>coronals</em> (constriction by the tongue tip/blade), the phonemes /(Begin IPA) T, , D (End IPA)/ are both coronal stops, and the conditioned variant [(Begin IPA) fishhook (End IPA)] is a coronal continuant.</p>
<p>The distinctive feature idea was explored further by the Prague School of phonologists, notably Roman Jakobson and Nikolai Trubetzkoy, for whom <em>contrast</em> between phonemes was a core theoretical premise. Analyzing the systematicity of such contrasts required viewing phonemes as possessing features distinguishing each from the others. The particular features necessary to distinguish phonemes describe the content of that phoneme. In this sense,</p>
<p><span>[a]</span>ny minimal distinction carried by the message confronts the listener with a two-choice situation. Within a given language each of these oppositions has a specific property which differentiates it from all the others. The listener is obliged to choose either between two polar qualities of the same category [<span class="math inline">…</span>] or between the presence and absence of a certain quality [<span class="math inline">…</span>]. The choice between the two opposites may be termed <em>distinctive feature</em>. The distinctive features are the ultimate distinctive entities of language since no one of them can be broken down into smaller linguistic units. The distinctive features combined into one simultaneous or [<span class="math inline">…</span>] concurrent bundle form a <em>phoneme</em>. <span class="citation">(Jakobson et al. 1952, 3)</span></p>
<p>Phonological features were intended to be the cognitive connection between the articulatory and perceptual speech systems.</p>
<p><span>[</span>T]he distinctive features correspond to controls in the central nervous system which are connected in specific ways to the human motor and auditory systems. In speech perception detectors sensitive to the property detectors [<span class="math inline">…</span>] are activated, and appropriate information is provided to centers corresponding to the distinctive feature[s] [<span class="math inline">…</span>]. This information is forwarded to higher centers in the nervous system where identification of the utterance takes place. In producing speech, instructions are sent from higher centers in the nervous system to the different feature[s] [<span class="math inline">…</span>] about the utterance to be produced. The features then activate muscles that produce the states and configurations of different articulators[.] <span class="citation">(Halle 1983, 95)</span></p>
<p>Over a quarter century later, the same idea informs the neurolinguistics and biolinguistics literatures.</p>
<p>The [<span class="math inline">…</span>] featurally specified representation constitutes the format that is both the endpoint of perception — but which is also the set of instructions for articulation. <span class="citation">(Poeppel and Idsardi 2011, 179)</span></p>
<p>Features serve as the cognitive basis of the bi-directional translation between speech production and perception, and are part of the long-term memory representation for the phonological content of morphemes, thus forming a memory-action-perception loop [<span class="math inline">…</span>] at the lowest conceptual level. <span class="citation">(Volenec and Reiss 2017, 270)</span></p>
<p>Just as a phonemic analysis of a language’s phonological patterns reveals its phonemic inventory, so does a featural analysis reveal its distinctive feature inventory. A phoneme consists of an individual combination of distinctive feature values, but there will be some particular distinctive feature value combinations that do not correspond to phonemes in a given language. For example, the phonemic inventory of Russian includes voiceless oral stops /(Begin IPA) P, , T, , K (End IPA)/ and voiced oral stops /(Begin IPA) B, , D, , G (End IPA)/ at the same three places of articulation (labial, coronal, dorsal), but nasal stops /(Begin IPA) M, , N (End IPA)/ at only two of these — meaning that the combination of distinctive feature values describing a dorsal nasal /(Begin IPA) engma (End IPA)/ does not correspond to a phoneme in Russian.</p>
<p>A phonemic inventory is often presented as a stand-alone entity in the context of a phonological analysis, but phonologists recognize restrictions on the distributions of individual phonemes (see e.g. <span class="citation">Hall (2013)</span>). For example, some argue is a phoneme of English, but it is only found word-finally (e.g. [(Begin IPA) S, I, engma (End IPA)] ‘sing’), before word-level suffixes (e.g. [(Begin IPA) S, I, engma, schwa, ˞ (End IPA)] ‘singer’, [(Begin IPA) S, I, engma, I, engma (End IPA)] ‘singing’), or when followed by K or G ([(Begin IPA) S, I, engma, K (End IPA)] ‘sink’, [(Begin IPA) F, I, engma, G, schwa, ˞ (End IPA)] ‘finger’). Similarly, /(Begin IPA) eth (End IPA)/ is a phoneme of English, but its distribution is heavily restricted, being found at the beginning of a handful of function words, mostly determiners ( ‘this, that, these, those, them, thee, thy, the, then’), in a handful of words ending in [(Begin IPA) eth, schwa, ˞ (End IPA)] (‘other, bother, weather, feather, mother, brother, father’), and at the end of a handful of verbs, mostly denominal (‘breathe, bathe, sheathe, writhe’).</p>
<p>Conditioned variants of phonemes by definition also have restricted distributions. The distinction in Russian between voiced and voiceless obstruents (a class that includes the stops noted above) is found only before sonorants, and is otherwise <em>neutralized</em> in agreement with following obstruents (voiced before voiced, voiceless before voiceless) or to voiceless word-finally.</p>
<p>These restrictions on the distributions of phonemes and of their conditioned variants, and those on the recombination of distinctive features mentioned earlier, provide fodder for the kinds of abstract analyses that command particular attention in phonology, to which we now turn.</p>
<h1 id="sec:how">How Abstract are Mental Representations?</h1>
<p>The question posed in this section title mainly lurks under the surface of modern debates in phonological theory, such as the extent to which phonology can be reduced to physiological principles governing articulation and perception <span class="citation">(Ohala 1981; Ohala 1997; Hale and Reiss 2000; Hayes, Kirchner, and Steriade 2004; Blevins 2004; Heinz and Idsardi 2013; Reiss 2018)</span>. The question was asked more directly by <span class="citation">Kiparsky (1968)</span>, as directly betrayed by this classic paper’s title: <em>How abstract is phonology?</em> The paper addresses concerns about early work in generative phonology, which admitted the possibility of many transformations applying in crucial sequence from underlying to surface representations such that the specifications of a given underlying representation could be quite a bit different from those of its eventual surface representation.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> The concerns were about the possibility of ‘excessive abstractness’, though different types of abstractness and how one is to measure them with respect to one another were often more matters of opinion than principle. As discussed further below, <span class="citation">Kiparsky (1968)</span>’s proposal was not so much a line drawn in the sand as it was the curtailment of one particular type of abstractness. His and subsequent theoretical proposals have placed limits on the <em>possible types of differences</em> that may hold between an underlying representation and its various surface manifestations, but they have not somehow placed limits on the <em>‘distance’</em> between these representations.</p>
<p>The form of the evidence for a relatively abstract phonological analysis is typically one where what otherwise appears to be the same phoneme exhibits two distinctive forms of behavior, one expected (call this one <span class="math inline"><em>A</em></span>) and one unexpected (call this one <span class="math inline"><em>B</em></span>). The more strands of evidence of this sort that exist in any given analysis, the more compelling the case for it; Yokuts <span class="citation">(Kisseberth 1969)</span>, Nupe <span class="citation">(Hyman 1970)</span>, and Dida <span class="citation">(Kaye 1980)</span> are particularly compelling cases. The unexpected behavior of <span class="math inline"><em>B</em></span> can be approached in one of two ways, both being abstract in some sense.</p>
<p>One approach is to posit that <span class="math inline"><em>A</em></span> and <span class="math inline"><em>B</em></span> are indeed phonemically identical, but that there is some abstract non-phonological diacritic marking <span class="math inline"><em>X</em></span>, present in forms with <span class="math inline"><em>B</em></span> but not in those with <span class="math inline"><em>A</em></span>, to which relevant phonological transformations are sensitive. (<span class="math inline"><em>X</em></span> may but need not have other functions in the language.) We call this the <em>abstract diacritic</em> approach.</p>
<p><span class="citation">Kiparsky (1968)</span> refers to the other approach as “the diacritic use of phonological features”, and it is instantiated in one of two basic ways. One instantiation is to directly posit that <span class="math inline"><em>B</em></span> is a phoneme distinct from <span class="math inline"><em>A</em></span> in terms of some feature such that the combination of distinctive features describing <span class="math inline"><em>B</em></span> does not surface, either at all or in the phonological contexts or lexical items in question. Relevant phonological transformations sensitive to the <span class="math inline"><em>A</em>/<em>B</em></span> distinction apply, and this distinction is subsequently neutralized with <span class="math inline"><em>A</em></span>. The other instantiation is to posit that <span class="math inline"><em>A</em></span> and <span class="math inline"><em>B</em></span> are phonemically identical, but that there is some other phoneme <span class="math inline"><em>C</em></span>, co-present with <span class="math inline"><em>B</em></span> but not with <span class="math inline"><em>A</em></span>, to which relevant phonological transformations are sensitive. <span class="math inline"><em>C</em></span> is either subsequently deleted or neutralized with some other phoneme. We call both of these instantiations the <em>abstract phoneme</em> approach.</p>
<p>Abstract phonemes can be either absolute or restricted. An <em>absolutely abstract phoneme</em> — or an “imaginary segment” <span class="citation">(Crothers 1971)</span> — is a combination of distinctive feature values posited as a phoneme in some set of morphemes but that is not realized as such in the surface forms of <em>any morphemes in the language</em>. An example appears in an analysis of exceptions to vowel harmony in Hungarian <span class="citation">(Vago 1976)</span>: some stems with the neutral front vowels [(Begin IPA) I, , I, long mark, , E, long mark (End IPA)] unexpectedly condition backing of suffix vowels, motivating the postulation of abstract back vowel phonemes /(Begin IPA) turned M, , turned M, long mark, , baby gamma, long mark (End IPA)/ in these stems that are eventually and uniformly fronted.</p>
<p>A <em>restrictedly abstract phoneme</em> <span class="citation">(O’Hara 2017)</span> is a combination of distinctive feature values posited as a phoneme in a restricted set of contexts but that is not realized as such in <em>any surface contexts corresponding to that set</em>. An example appears in an analysis of exceptions to vowel coalescence in Mushunguli <span class="citation">(Hout 2017)</span>: some stems beginning in the high vowels [(Begin IPA) I, , U (End IPA)] unexpectedly block otherwise regular coalescence with preceding /(Begin IPA) A (End IPA)/, motivating the postulation of glide phonemes /(Begin IPA) J, , W (End IPA)/ that respectively never surface before [(Begin IPA) I, , U (End IPA)] but that do surface in other contexts. These glides block coalescence and are subsequently deleted just before .</p>
<p>All abstract phoneme analyses crucially rely on the possibility of <em>opaque interactions</em> between phonological transformations <span class="citation">(Kiparsky 1973)</span>. In each of the examples sketched above, the transformation that voids the representation of its abstract phoneme crucially must not apply before application of the relevant transformation(s) sensitive to the abstract phoneme.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> In some cases, the abstract phoneme is needed in the representation to prevent the application of an otherwise applicable transformation, as in the case of Mushunguli. These involve the type of opaque interaction known as <em>counterfeeding</em>. In other cases, the abstract phoneme is needed in the representation to ensure the application of an otherwise inapplicable transformation, as in the case of Hungarian. These involve the type of opaque interaction known as <em>counterbleeding</em>. Abstractness is thus intimately intertwined with opacity: to the extent that a theoretical framework (dis)allows opaque interactions, it (dis)allows abstract phoneme analyses.</p>
<p><span class="citation">Kiparsky (1968)</span>’s proposed principle was meant to ensure that every phoneme specified as a particular combination of distinctive features in a given morpheme’s underlying form will be realized with that precise set of specifications in at least one surface form of that morpheme.<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> This effectively excludes abstract phoneme analyses such as those sketched above — but in each of these cases the alternative is to rely instead on diacritic marking, which is, as already noted, also a form of abstractness. More recently, <span class="citation">O’Hara (2017)</span> specifies a MaxEnt learning model that assigns sufficiently high probability to a restrictedly abstract phoneme analysis (based on a case in Klamath), and significantly lower probability to both absolutely abstract phoneme and abstract diacritic alternatives. It is thus reasonably clear that a hypothetical abstract-o-meter (recall Figure [fig:degree]) would place absolutely abstract phonemes closer to the ‘too abstract’ end of the spectrum than restrictedly abstract phonemes, but where abstract diacritics fall on that spectrum is a largely unresolved matter. We must thus conclude here that “a phonological analysis, independently of its ‘degree’ of abstractness, is (only) as adequate as the motivation and evidence that can be produced in favor of it and against substantive alternatives” <span class="citation">(Baković 2009, 183)</span>.</p>
<p>This section and the last focused on the issue of phonological abstractness in the mental lexicon at the level of the phoneme and of the distinctive feature. This is primarily because most relevant research has been focused here, with the possible exception of the syllable <span class="citation">(Goldsmith 2011; Strother-Garcia 2019)</span>. However, other representational levels and systems also have been argued to play a role in the mental representations of words, notably tonal representations <span class="citation">(Yip 2002; Jardine 2016)</span> and metrical stress <span class="citation">(Hayes 1995; Hulst 2013)</span>. These representations have also been the subject of intense study, including the psycholinguistic and neurolinguistic literature, to which we turn in the next section.</p>
<h1 id="types-of-evidence">Types of Evidence</h1>
<p>The preceding sections discussed the perspective of phonological abstractness using structural linguistic arguments from the typology of spoken language. However, it is often claimed that structural arguments are far from a definitive proof, constituting one type of evidence. This is especially true for the psychological reality of these abstract forms. <span class="citation">Ohala (1974)</span> makes this point:</p>
<p>It seems to me that the important question should not be whether phonology is abstract or concrete, but rather what evidence there is for the psychological reality of a particular posited underlying form. If our aim is simply a descriptive one, then of course abstract forms can be posited without any further justification than the fact that they make it easier to state certain regularities. However, if we are interested in reflecting the competence of a native speaker, then we must provide evidence that what we are claiming as part of a speaker’s knowledge is indeed such.</p>
<p>What other types of evidence are there? This section describes various other types of evidence bearing on the question of abstractness. The first type of evidence comes from expanding the typology to consider non-spoken language, that is, signed and tactile language. This evidence is still structural, but provides an important window into the lexicon. How dependent is the long-term memory representation of a word on the physical system which externalizes it? At the same time, Ohala and many others take behavioral and psychological tests as providing additional necessary, if not sufficient, evidence for the reality of these forms. Recent advances in neuroimaging and increasing ease of use allow for intricate looks into the biological underpinnings of phonological representations, combined with computational models and simulations. However, literature in all these topics is vast, especially with regard to experimental work. Here we provide a sample of work that bears on the questions described earlier.</p>
<h2 id="evidence-from-signed-and-tactile-phonology">Evidence from Signed and Tactile Phonology</h2>
<p>An important source of evidence for the existence and nature of phonological abstraction comes from languages without a spoken modality - namely, signed and tactile languages. Sign languages arise spontaneously in Deaf communities, are acquired during childhood through normal exposure without instruction, and exhibit all of the facets and complexity found in spoken languages (see <span class="citation">Sandler and Lillo-Martin (2006)</span> for a groundbreaking overview). However, <span class="citation">Sandler (1993)</span> argues that if human language evolved without respect to modality, we should find hearing communities which just happen to use sign language rather than spoken language, and we do not. Sign language, she argues, is thus “an adaptation of existing physical and cognitive systems for the purpose of communication among people for whom the auditory channel is not available&quot;.</p>
<p>Sign languages offer, as <span class="citation">Sandler (1993)</span> puts it, “a unique natural laboratory for testing theories of linguistic universals and of cognitive organization.&quot; They give insight into the contents of phonological form, and conditions on which aspects of grammar are amodal and which are tied to the modality. They also offer unique opportunities to study the emergence of phonology within a developing grammar <span class="citation">(Goldin-Meadow 2005; Senghas, Kita, and Özyürek 2004; Marsaja 2008; Sandler et al. 2005)</span>.</p>
<p>One crucial contribution of non-spoken phonology is that it switches the issue of “how abstract is phonology?&quot; to “where does abstraction lie, and to what extent is it independent of the modality?&quot; There are generally two directions answers can take. To the extent that a given phonological form or constraint is present across modalities, one may make the case that it is truly abstract, in the sense that it exists without regard to the articulatory system which realizes it. On the other hand, to the extent that a given form or constraint differs, one can ascribe that difference to the modality, to the nature of the articulatory system. In this way, non-spoken phonology provides nuance into the relationship between the pressures of abstraction and the pressures of realization in the mental lexicon.</p>
<p>Sign languages, despite their rich history, were almost totally dismissed as natural and structured languages until the latter half of the 20th century (see <span class="citation">Hulst (\noop{3001}to appear)</span>, to appear for the history of sign phonology). <span class="citation">Stokoe (1960)</span> divided signs into meaningless chunks, showing that sign languages display both morpho-syntactic and phonological levels, a “duality of patterning&quot; or “double articulation&quot; considered previously as a unique property of spoken languages <span class="citation">(Martinet 1960; Hockett 1960)</span>. Stokoe’s phonological system specified abstract representations for parts of the sign: the handshape, the movement of the hand, and the location in front of or on the body. <span class="citation">Hulst (\noop{3001}to appear)</span> notes that Stokoe’s division of signed forms was designed for transcription, but he regarded these symbols as abstract representations of the discrete units characterizing the signs.</p>
<p>Much work on the psychological reality of these phonological abstractions in sign came from the Salk Institute (see <span class="citation">Klima and Bellugi (1979)</span> for an accessible overview, and <span class="citation">Emmorey (2001)</span> for more recent developments). Studying production errors, they demonstrated compositionality in both perception and production of sign forms. The Bellugi &amp; Klima group showed that cross-linguistically, signers make acceptability judgments about what they consider well-formed or ill-formed, evidence that they possess intrinsic knowledge of how these smaller units can be combined. As <span class="citation">Hulst (\noop{3001}to appear)</span> (to appear) notes, <span class="citation">Klima and Bellugi (1979)</span>’s accessibility and cognitive scope convinced many linguists and non-linguists of the importance of sign language, and sign linguistics as a proper study in ways Stokoe’s analysis was unable to.</p>
<p>These works galvanized the phonological study of signs, increasingly focused on sequential aspects of signs and away from simultaneous aspects, which Stokoe had emphasized <span class="citation">(Liddell and Johnson 1989; Liddell 1984; Newkirk 1981; Supalla and Newport 1978)</span>. Researchers discovered, among other evidence, the salience of the beginning and end points of the movement of signs for inflectional purposes where referents are marked by discrete spatial locations, as well as phonological processes like metathesis, a switch in the beginning and end point of the movement (see <span class="citation">Hulst and Kooij (2018)</span> for discussion).</p>
<p>The notion of both simultaneous and compositional structure in a sign, as well as sequential structure, raises a big question: how modality-dependent are these properties? Languages in both modalities have sequential and simultaneous structure, but exhibit differences in relative centrality of such structure. Spoken languages vary in syllable structure, word length, and stress patterns among syllables. Sign languages appear limited in all these aspects. They are overwhelmingly monosyllabic, have no clusters, and show extremely simple stress patterns, due to few polysyllabic words apart from fully reduplicated forms (see <span class="citation">Sandler and Lillo-Martin (2006)</span> for general discussion, and <span class="citation">(Wilbur 2011)</span> for discussion of signed syllables).</p>
<p>A further complication arises from the fact that sequential phonological structure in sign language appears mostly from morphosyntactic operations concatenating morphemes and words (e.g. affixation, compounding, and cliticization) <span class="citation">(Aronoff, Meir, and Sandler 2005)</span>. In general, sequential affixation is rare across sign languages <span class="citation">(Aronoff, Meir, and Sandler 2005)</span>, and sign exhibits a strong tendency to express concatenative morphology through compounding <span class="citation">(Meir 2012)</span>. <span class="citation">Aronoff, Meir, and Sandler (2005)</span> show that affixation usually emerges from the grammaticalization of free words, through a series of diachronic changes affecting both phonological and semantic factors. They cite the relative youth of sign languages as a major factor in their lack of affixes. No known sign languages are over 300 years old, with some like Nicaraguan Sign Language, as young as 40 <span class="citation">(Woll, Sutton-Spence, and Elton 2001)</span>, and many others in development.</p>
<p>The curious lack of sequential structure in sign languages does not imply structural degeneracy or simplicity, however. Sign languages routinely demonstrate nonconcatenative morphology <span class="citation">(Sandler 1989; Meier 2002)</span>, incorporating morphological material simultaneously in the phonology alongside the restricted sequential form. Simultaneous phonological structure exists in all languages, but differ across modalities in the amount. For example, while the simultaneous ‘autosegmental’ representations for tone or harmony patterns <span class="citation">(J. Goldsmith 1976)</span> typically consist of one or two features, the autosegmental representation of hand configuration alone in sign language contains around half of the distinctive features comprising a sign organized in an intricate feature geometry <span class="citation">(Hulst 1995; Sandler 1996)</span>. Such tradeoffs in sequential and simultaneous centrality have been argued to stem from a computational restriction that may be realized via different representations in different modalities <span class="citation">(Rawski 2017)</span>.</p>
<p>This converging line of evidence suggests that the phonological grammar may leverage the representational abilities of the particular articulatory/perceptual system. <span class="citation">Brentari (2002)</span> and <span class="citation">Emmorey (2001)</span> argue that visual perception of signs (even with sequential properties) is more “instantaneous&quot; than auditory speech perception. This leads <span class="citation">Hulst and Kooij (2018)</span> to adapt <span class="citation">J. A. Goldsmith (1976)</span>’s division of phonology in terms of the notions of “vertical and horizontal slicing of the signal&quot;. They state:</p>
<p>an incoming speech signal is first spliced into vertical slices, which gives rise to a linear sequence of segments. Horizontal slicing then partitions segments into co-temporal feature classes and features. In the perception of sign language, however, the horizontal slicing takes precedence, which gives rise to the simultaneous class nodes that we call handshape, movement, and place. Then, a subsequent vertical slicing of each of these can give rise to a linear organization.</p>
<p>Perhaps the most intriguing evidence for or against abstractness may come from studying the degree to which the phonetics and phonology differs across modalities. <span class="citation">Lillo-Martin (1997)</span> cites <span class="citation">Blakemore (1974)</span>’s result that exposure to vertical and horizontal lines in the environment affects development of feline visual perception, and asks “why shouldn’t exposure to the special acoustic properties of the modality affect perception, especially auditory perception?&quot; <span class="citation">Sandler and Lillo-Martin (2006)</span> note, for example, that unlike spoken syllables in many languages, sign syllables prohibit location clusters analogous to consonant clusters, as well as diphthong-like movement clusters, and sign physiology requires movement between locations. They additionally note that sign syllables do not have onset-rhyme asymmetries, which affects syllable structure and stress assignment. Many more such differences have been studied, and further work in this area will bring important issues to bear on the nature of abstract representations in and across articulatory systems.</p>
<p>The similarities and differences in phonological abstraction across modalities means signed languages continue to play an important role as evidence of abstraction in the lexicon. This holds equally true for language expressed by the DeafBlind through the tactile modality, often called tactile or pro-tactile sign languages (see <span class="citation">Edwards (2014)</span> for a recent phonological analysis).</p>
<h2 id="psycholinguistic-and-neurolinguistic-evidence">Psycholinguistic and neurolinguistic evidence</h2>
<p>As mentioned, behavioral testing has long been argued as necessary evidence for the mental reality of phonological abstraction, in addition to typological evidence. The introduction and improvement of neuroimaging methods enabled correlations between behavioral tasks and gross neural excitation levels associated with them. In addition, recent simulation tools allow for modeling of phonological representations in an idealized <em>in silico</em> setting. Here we overview several results via behavioral and neural methods bearing on phonological organization of the lexicon by its salient features into abstract phonemes, as well as work on the temporal abstraction of speech.</p>
<p>One salient question concerns experimental evidence for abstract phonemic and featural representations of words. For example, In Russian, the sounds [(Begin IPA) D (End IPA)] and [(Begin IPA) T (End IPA)], which featurally differ in voicing, are contrastive, members of different phonemes. In Korean, these sounds do not contrast and are members of a single phoneme. <span class="citation">Kazanina, Phillips, and Idsardi (2006)</span> used the neuroimaging method of magnetoencephalography (MEG), which tracks the timecourse of gross, large-scale neural activity, to show that Russian and Korean speakers react differently to these sounds. The Russian speakers separated the sounds into two categories corresponding to /(Begin IPA) T (End IPA)/ and /(Begin IPA) D (End IPA)/. On the other hand, the Korean speakers did not separate the sounds, again corresponding to the analysis of a single underlying phoneme. from this result, the authors conclude that both phonetic and abstract phonemic analyses necessarily shape the perceptual analysis of speech sounds.</p>
<p>There is much evidence supporting a vast neuronal ensemble for phonological representations in speech production and perception (see <span class="citation">Eickhoff et al. (2009)</span> and <span class="citation">Jueptner and Krukenberg (2001)</span> for an extensive overview). In particular, various portions of the superior temporal sulcus are suggested to encode the phonological representations discussed in this chapter. <span class="citation">Scharinger, Idsardi, and Poe (2011)</span> used a combination of MEG imaging and statistical modeling to map the entire vowel space of a language (Turkish) onto three-dimensional cortical space, organized by lateral-medial, anterior–posterior, and inferior-superior axes. Their statistical model comparisons showed that, while cortical vowel maps do reflect acoustic properties of the speech signal, articulator-based and featural speech sound information “warps the acoustic space toward linguistically relevant categories&quot;.</p>
<p><span class="citation">Scharinger, Monahan, and Idsardi (2012)</span> used MEG to localize three vowel feature variables (height, frontness and roundness) to the superior temporal gyrus. <span class="citation">Mesgarani et al. (2014)</span> used cortical electrode placement to show that the Superior Temporal Sulcus encodes a ‘manner of articulation’ parameter of speech sounds. Intriguingly, different electrodes responded selectively to stops, sibilant fricatives, low back vowels, high front vowels and a palatal glide, and nasals, respectively. <span class="citation">Bouchard et al. (2013)</span> showed similar results, that the Superior Temporal Gyrus encodes a ‘place of articulation’ parameter, confirming labial, coronal and dorsal place features, across various manner classifications. These results match with <span class="citation">Hickok and Poeppel (2007)</span>’s hypothesis that “the crucial portion of the STS that is involved in phonological-level processes is bounded anteriorly by the most anterolateral aspect of Heschl’s gyrus and posteriorly by the posterior-most extent of the Sylvian fissure”.</p>
<p>Additionally, recent experimental evidence using aphasic patients even supports the existence of abstract phonological rules in processing. Linguistic analysis posits that the English words <em>pit</em> and <em>spit</em> both contain the segment /(Begin IPA) P (End IPA)/ in their underlying representations. In the surface representation <em>pit</em>, with the /(Begin IPA) P (End IPA)/ on word-initial position, aspirates to get [(Begin IPA) P, H (End IPA)] and <em>spit</em> (preceded by /(Begin IPA) S (End IPA)/) does not, to get [(Begin IPA) P (End IPA)]. <span class="citation">Buchwald and Miozzo (2011)</span> constructed an experiment using the productions of two aphasic patients who were unable to produce an /(Begin IPA) S (End IPA)/ in relevant consonant clusters like /(Begin IPA) S, P (End IPA)/ or /(Begin IPA) S, T (End IPA)/, and compared them with correctly produced consonants. They wanted to test whether an aphasic would aspirate the /(Begin IPA) P (End IPA)/, (marking phonological fricative-deletion), or not (the fricative deleted <em>after</em> successful application of the phonological rule).</p>
<p>To analyze it, they compared the voice-onset-time (VOT) of the two patients on the two instances. VOT provides an acoustic measure of the relative aspiration of the consonant by seeing how much the following voicing is delayed. One patient had a long VOT [(Begin IPA) P, H (End IPA)] while the other had a short VOT [(Begin IPA) P (End IPA)], confirming the divide between two distinct levels of phonological and phonetic influences in processing. Follow-up work by <span class="citation">Buchwald and Miozzo (2012)</span> showed similar results for nasal consonants which deleted in clusters like /(Begin IPA) S, N (End IPA)/ and /(Begin IPA) S, M (End IPA)/. The conclusion to be drawn from these studies points to abstraction in the units being processed, mentally divorced from their phonetic realization but ultimately driving it.</p>
<p>Apart from the biological underpinmnings of the atomic representations characterizing speech units, there is additionally much work focused on the biological underpinnings of temporal abstractions in speech. Specifically, much of this work focuses on the insula, basal ganglia, and cerebellum, where temporal information is speculated to be filtered in a cortical-subcortical loop for the purposes of motor planning <span class="citation">(Eickhoff et al. 2009)</span>. In this process, motor sequence plans are filtered through through basal ganglia, while the cerebellum converts the sequences into fluent, temporally distributed articulations. <span class="citation">Ackermann, Mathiak, and Riecker (2007)</span> underpin this by describing drastic negative effects on speech production consistent with damage to the cerebellum.</p>
<p>While neuroimaging methods brought many advantages, they simultaneously introduced a uniquely thorny issue to phonological abstraction, which <span class="citation">(Poeppel 2012)</span> divides into the “Maps Problem&quot; and the “Mapping Problem&quot;. The Maps Problem concerns descriptive analysis of the behavioral and neural underpinnings of mental representations, say the effects and brain areas associated with a particular phonological phenomenon. The Mapping Problem concerns how to take a particular mental representation, perhaps known to correlate with some behavioral entity or brain network, and mechanistically connect it to neuronal function. Neither the maps problem nor the mapping problem have easy answers <span class="citation">(Buzsaki 2019)</span>, and decades of work have led to many open and nuanced questions.</p>
<p>Attempting to address the Mapping Problem, some work seeks a somewhat more explanatory approach to the forces underlying the temporal segmentation of the signal produced and perceived. One emerging insight is that the perceptual system divides an incoming auditory stream into two distinct time widows <span class="citation">(Giraud and Poeppel 2012; Chait et al. 2015)</span>. What phonological entities do they map to? As <span class="citation">Poeppel and Idsardi (2011)</span> put it, there are:</p>
<p>Two critically important windows that appear instantiated in spoken languages: segments and syllables. Temporal coordination of distinctive features overlapping for relatively brief amounts of time (10–80 ms) comprise segments; longer coordinated movements (100–500 ms) constitute syllabic prosodies. <span class="citation">(Poeppel and Idsardi 2011, 182)</span></p>
<p>A more fundamental question concerns the neural mechanism which drives these windows and their coordination. Oscillatory activity within and between neural populations has been posited <span class="citation">(Giraud and Poeppel 2012)</span>. Neural populations which comprise a certain type of neuron may show a stable neural oscillation at certain frequency bands which varies depending on their excitatory and inhibitory properties (see <span class="citation">(Buzsaki 2006)</span> for an accessible overview). Evidence suggests that pyramidal interneuron gamma oscillations, as well as theta oscillations, comprise the segmental vs. syllabic time distinction. These two oscillations funnel an incoming speech signal into time windows of different sizes, computationally represented by the waveform of the population activity.</p>
<p><em>In silico</em> modeling work reveals another interesting property. While these oscillations do track the signal, a crucial feature is that rhythms of distinct frequencies show specific coupling properties, termed cross-frequency coupling <span class="citation">(Hyafil, Giraud, et al. 2015)</span>. Briefly, this means that the stable oscillatory populations innervate each other, allowing different timescales to track one another, effectively parsing a temporally complex signal efficiently. Specifically for speech perception, <span class="citation">Hyafil, Fontolan, et al. (2015)</span> showed that when a network exhibiting gamma oscillations coupled to a network showing theta osciallations, it was able to effectively segment a corpus of phonological words much better than a network where this coupling was absent.</p>
<p>These results reflect an explosion of work using neurolinguistic and psycholinguistic tests to describe the sort of representations speakers have. The intersection of experimental results with theory promises many new insights into the mental content of the lexicon (see Poeppel &amp; Sun, this volume). For a further dicussion on the particular biological substrate underlying phonological abstraction, and how they impact the phonetics-phonology interface, see <span class="citation">Volenec and Reiss (2017)</span>.</p>
<h1 id="conclusion">Conclusion</h1>
<p>In this chapter, we have endeavored to motivate and review a central idea of modern generative phonology that the fundamental representational units of words in languages are abstract and psychologically real. In particular the systematic patterning of the pronunciation of morphemes motivates an abstract mental representation of a morpheme’s pronunciation. These underlying forms are largely regarded as sequences of phonemes, which are themselves abstractions, and which are organized along featural dimensions. These abstract mental representations find support not only from the patterning in morpho-phonological paradigms, but also from language change, from sign language linguistics, and from psycholinguistic and neurolinguistic study.</p>
<p>There are many open and fascinating questions regarding the nature of abstract mental representations of words. Many are among the most basic and fundamental. How abstract can they be? How are they learned? How are they realized in the brain? The fact that we simultaneously know both so much and so little about phonological abstractness in the mental lexicon sends a clear message that this will continue to be a fertile and exciting area of research for many years to come.</p>
<p>To conclude this chapter, we can do no better than to repeat the concluding sentences of <span class="citation">Labov (2020, 57, emphasis added)</span>: “We have a common ground in our understanding of what it means to know a language. It involves knowing a vast number of particular things. <em>But at bottom it is reaching down to something very deep, very abstract, and very satisfying.</em>”</p>
<h1 id="acknowledgments">Acknowledgments</h1>
<p>We thank Aniello De Santo, Charles Reiss, Veno Volenec, and two anonymous reviewers for valuable feedback on an early draft.</p>
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-ackermann2007contribution">
<p>Ackermann, Hermann, Klaus Mathiak, and Axel Riecker. 2007. “The Contribution of the Cerebellum to Speech Production and Speech Perception: Clinical and Functional Imaging Data.” <em>The Cerebellum</em> 6 (3). Springer: 202–13.</p>
</div>
<div id="ref-Anderson1985">
<p>Anderson, Stephen. 1985. <em>Phonology in the Twentieth Century</em>. The University of Chicago Press.</p>
</div>
<div id="ref-aronoff2005paradox">
<p>Aronoff, Mark, Irit Meir, and Wendy Sandler. 2005. “The Paradox of Sign Language Morphology.” <em>Language</em> 81 (2). NIH Public Access: 301.</p>
</div>
<div id="ref-bakovic1994spanish">
<p>Baković, Eric. 1994. “Strong onsets and Spanish fortition.” In <em>Proceedings of the 6th Student Conference in Linguistics</em>, edited by Giordano Chris and Daniel Ardron, 21–39. Cambridge, MA: MIT Working Papers in Linguistics.</p>
</div>
<div id="ref-Bakovic09">
<p>———. 2009. “Abstractness and motivation in phonological theory.” <em>Studies in Hispanic and Lusophone Linguistics</em> 2: 183–98.</p>
</div>
<div id="ref-Bakovic-elsewhere">
<p>———. 2013. <em>Blocking and Complementarity in Phonological Theory</em>. London: Equinox.</p>
</div>
<div id="ref-Bakovic-Blumenfeld-AMP17">
<p>Baković, Eric, and Lev Blumenfeld. 2017. “A set-theoretic typology of phonological map interaction.”</p>
</div>
<div id="ref-sep-multiple-realizability">
<p>Bickle, John. 2020. “Multiple Realizability.” In <em>The Stanford Encyclopedia of Philosophy</em>, edited by Edward N. Zalta, Summer 2020. <a href="https://plato.stanford.edu/archives/sum2020/entries/multiple-realizability/" class="uri">https://plato.stanford.edu/archives/sum2020/entries/multiple-realizability/</a>; Metaphysics Research Lab, Stanford University.</p>
</div>
<div id="ref-blakemore1974developmental">
<p>Blakemore, COLIN. 1974. “Developmental Factors in the Formation of Feature Extracting Neurons.” <em>The Neurosciences: Third Study Program</em>. Cambridge, Mass.: The MIT Press, 105–13.</p>
</div>
<div id="ref-Blevins2004">
<p>Blevins, Juliette. 2004. <em>Evolutionary Phonology</em>. Cambridge University Press.</p>
</div>
<div id="ref-Bloch1941">
<p>Bloch, Bernard. 1941. “Phonemic Overlapping.” <em>American Speech</em> 16 (4): 278–84. doi:<a href="https://doi.org/10.2307/486567">10.2307/486567</a>.</p>
</div>
<div id="ref-bloomfield1933">
<p>Bloomfield, Leonard. 1933. <em>Language</em>. New York: H. Holt; Co.</p>
</div>
<div id="ref-bouchard2013functional">
<p>Bouchard, Kristofer E, Nima Mesgarani, Keith Johnson, and Edward F Chang. 2013. “Functional Organization of Human Sensorimotor Cortex for Speech Articulation.” <em>Nature</em> 495 (7441). Nature Publishing Group: 327.</p>
</div>
<div id="ref-brentari2002modality">
<p>Brentari, Diane. 2002. “Modality Differences in Sign Language Phonology and Morphophonemics.” <em>Modality and Structure in Signed and Spoken Languages</em>. Cambridge University Press Cambridge, UK, 35–64.</p>
</div>
<div id="ref-buchwald2011finding">
<p>Buchwald, Adam, and Michele Miozzo. 2011. “Finding Levels of Abstraction in Speech Production: Evidence from Sound-Production Impairment.” <em>Psychological Science</em> 22 (9). Sage Publications Sage CA: Los Angeles, CA: 1113–9.</p>
</div>
<div id="ref-buchwald2012phonological">
<p>———. 2012. “Phonological and Motor Errors in Individuals with Acquired Sound Production Impairment.” <em>Journal of Speech, Language, and Hearing Research</em>. ASHA.</p>
</div>
<div id="ref-buzsaki2006">
<p>Buzsaki, Gyorgy. 2006. <em>Rhythms of the Brain</em>. Oxford University Press.</p>
</div>
<div id="ref-buzsaki2019">
<p>———. 2019. <em>The Brain from Inside Out</em>. Oxford University Press, USA.</p>
</div>
<div id="ref-chait2015multi">
<p>Chait, Maria, Steven Greenberg, Takayuki Arai, Jonathan Z Simon, and David Poeppel. 2015. “Multi-Time Resolution Analysis of Speech: Evidence from Psychophysics.” <em>Frontiers in Neuroscience</em> 9. Frontiers: 214.</p>
</div>
<div id="ref-Cheng2015">
<p>Cheng, Eugenia. 2015. <em>How to Bake Pi: An Edible Exploration of the Mathematics of Mathematics</em>. New York: Basic Books.</p>
</div>
<div id="ref-Coleman2002">
<p>Coleman, John. 2002. “Phonetic Representations in the Mental Lexicon.” In <em>Phonetics, Phonology and Cognition</em>, edited by J. Durand and B. Laks, 96–130. Oxford: Oxford University Press.</p>
</div>
<div id="ref-crothers1971">
<p>Crothers, J. 1971. “On the Abstractness Controversy.” UC Berkeley Department of Linguistics.</p>
</div>
<div id="ref-Dresher2011">
<p>Dresher, Elan B. 2011. “The Phoneme.” In <em>The Blackwell Companion to Phonology</em>, edited by Elizabeth Hume Marc van Oostendorp Colin J. Ewen and Keren Rice, 1:241–66. Malden, MA &amp; Oxford: Wiley-Blackwell.</p>
</div>
<div id="ref-edwards2014">
<p>Edwards, Terra. 2014. “Language Emergence in the Seattle Deafblind Community.” PhD thesis, University of California, Berkeley.</p>
</div>
<div id="ref-eickhoff2009systems">
<p>Eickhoff, Simon B, Stefan Heim, Karl Zilles, and Katrin Amunts. 2009. “A Systems Perspective on the Effective Connectivity of Overt Speech Production.” <em>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</em> 367 (1896). The Royal Society London: 2399–2421.</p>
</div>
<div id="ref-emmorey2001language">
<p>Emmorey, Karen. 2001. <em>Language, Cognition, and the Brain: Insights from Sign Language Research</em>. Psychology Press.</p>
</div>
<div id="ref-giraudpeoppel2012">
<p>Giraud, Anne-Lise, and David Poeppel. 2012. “Cortical Oscillations and Speech Processing: Emerging Computational Principles and Operations.” <em>Nature Neuroscience</em> 15 (4). Nature Publishing Group: 511.</p>
</div>
<div id="ref-goldinmeadow2005">
<p>Goldin-Meadow, Susan. 2005. <em>The Resilience of Language: What Gesture Creation in Deaf Children Can Tell Us About How All Children Learn Language</em>. Psychology Press.</p>
</div>
<div id="ref-goldsmith76">
<p>Goldsmith, John. 1976. “Autosegmental Phonology.” PhD thesis, MIT, Cambridge, MA.</p>
</div>
<div id="ref-Goldsmith2011">
<p>———. 2011. “The Syllable.” In <em>The Blackwell Handbook of Phonological Theory</em>, edited by John A. Goldsmith, Jason Riggle, and Alan C. L. Yu, 164–96. Wiley-Blackwell.</p>
</div>
<div id="ref-goldsmith1976autosegmental">
<p>Goldsmith, John A. 1976. <em>Autosegmental Phonology</em>. Vol. 159. Indiana University Linguistics Club Bloomington.</p>
</div>
<div id="ref-HaleReiss2000">
<p>Hale, Mark, and Charles Reiss. 2000. “Substance Abuse and Dysfunctionalism: Current Trends in Phonology.” <em>Linguistic Inquiry</em> 31: 157–69.</p>
</div>
<div id="ref-CurrieHall2013">
<p>Hall, Kathleen Currie. 2013. “A Typology of Intermediate Phonological Relationships.” <em>The Linguistic Review</em> 30 (2): 215–75.</p>
</div>
<div id="ref-Halle1983">
<p>Halle, Morris. 1983. “On Distinctive Features and Their Articulatory Implementation.” <em>Natural Language and Linguistic Theory</em> 1: 91–105. doi:<a href="https://doi.org/10.1007/BF00210377">10.1007/BF00210377</a>.</p>
</div>
<div id="ref-Halle2003">
<p>———. 2003. <em>From Memory to Speech and Back: Papers on Phonetics and Phonology 1954–2002</em>. Berlin, Boston: De Gruyter Mouton.</p>
</div>
<div id="ref-harris1969spanish">
<p>Harris, James W. 1969. <em>Spanish Phonology</em>. Cambridge, Mass.: Massachusetts Institute of Technology Press.</p>
</div>
<div id="ref-hayes1995">
<p>Hayes, Bruce. 1995. <em>Metrical Stress Theory: Principles and Case Studies</em>. University of Chicago Press.</p>
</div>
<div id="ref-Hayes2009">
<p>———. 2009. <em>Introductory Phonology</em>. Wiley-Blackwell.</p>
</div>
<div id="ref-pbp04">
<p>Hayes, Bruce, Robert Kirchner, and Donca Steriade, eds. 2004. <em>Phonetically-Based Phonology</em>. Cambridge University Press.</p>
</div>
<div id="ref-Heinz-Idsardi-2013-WCDRADL">
<p>Heinz, Jeffrey, and William Idsardi. 2013. “What Complexity Differences Reveal About Domains in Language.” <em>Topics in Cognitive Science</em> 5 (1): 111–31.</p>
</div>
<div id="ref-hickok2007cortical">
<p>Hickok, Gregory, and David Poeppel. 2007. “The Cortical Organization of Speech Processing.” <em>Nature Reviews Neuroscience</em> 8 (5). Nature Publishing Group: 393.</p>
</div>
<div id="ref-Hockett1960">
<p>Hockett, Charles. 1960. “The Origin of Speech.” <em>Scientific American</em> 203 (3): 88–97.</p>
</div>
<div id="ref-Hout2017">
<p>Hout, Katherine. 2017. “Exceptions to hiatus resolution in Mushunguli (Somali Chizigula).” In <em>Africa’s Endangered Languages: Documentary and Theoretical Approaches</em>, edited by Kandybowicz Jason and Harold Torrence. Oxford University Press.</p>
</div>
<div id="ref-van1995composition">
<p>Hulst, Harry van der. 1995. “The Composition of Handshapes.” <em>Trondheim Work. Papers</em> 23: 1–17.</p>
</div>
<div id="ref-Hulst2013-rhythm">
<p>———. 2013. “Representing Rhythm.” In <em>Word Stress: Theoretical and Typological Issues</em>, edited by Harry van der Hulst. Cambridge University Press.</p>
</div>
<div id="ref-vanderHulst2020">
<p>———. \noop{3001}to appear. “Sign Language Phonology.” In <em>The handbook of the history of phonology</em>, edited by Dresher Elan and Harry van der Hulst. Oxford: Oxford University press.</p>
</div>
<div id="ref-HulstKooij2018">
<p>Hulst, Harry van der, and Els van der Kooij. 2018. “Phonological Structure of Signs – Theoretical Perspectives.” In <em>The Routledge Handbook of Theoretical and Experimental Sign Language Research</em>, edited by Roland Pfau Josep Quer Villanueva and Annika Herrmann. Routledge.</p>
</div>
<div id="ref-hyafil2015speech">
<p>Hyafil, Alexandre, Lorenzo Fontolan, Claire Kabdebon, Boris Gutkin, and Anne-Lise Giraud. 2015. “Speech Encoding by Coupled Cortical Theta and Gamma Oscillations.” <em>Elife</em> 4. eLife Sciences Publications Limited: e06213.</p>
</div>
<div id="ref-hyafil2015neural">
<p>Hyafil, Alexandre, Anne-Lise Giraud, Lorenzo Fontolan, and Boris Gutkin. 2015. “Neural Cross-Frequency Coupling: Connecting Architectures, Mechanisms, and Functions.” <em>Trends in Neurosciences</em> 38 (11). Elsevier: 725–40.</p>
</div>
<div id="ref-Hyman-1970">
<p>Hyman, Larry. 1970. “How Concrete Is Phonology?” <em>Language</em> 46 (1): 58–76.</p>
</div>
<div id="ref-JakobsonEtAl1952">
<p>Jakobson, Roman, C. Gunnar, M. Fant, and Morris Halle. 1952. <em>Preliminaries to Speech Analysis</em>. MIT Press.</p>
</div>
<div id="ref-Jardine-2016-diss">
<p>Jardine, Adam. 2016. “Locality and Non-Linear Representations in Tonal Phonology.” PhD thesis, University of Delaware.</p>
</div>
<div id="ref-Johnson2006">
<p>Johnson, Keith. 2006. “Resonance in an Exemplar-Based Lexicon: The Emergence of Social Identity and Phonology.” <em>Journal of Phonetics</em> 34 (4): 485–99.</p>
</div>
<div id="ref-Joshi-Kiparsky79">
<p>Joshi, S. D., and Paul Kiparsky. 1979. “Siddha and asiddha in Paninian phonology.” In <em>Current Approaches to Phonological Theory</em>, edited by Daniel A. Dinnsen, 223–50. Bloomington, IN: Indiana University Press.</p>
</div>
<div id="ref-Joshi-Kiparsky06">
<p>———. 2006. “The extended <em>siddha</em>-principle.” <em>Annals of the Bhandarkar Oriental Research Institute 2005</em>, 1–26.</p>
</div>
<div id="ref-jueptner2001motor">
<p>Jueptner, Markus, and Michael Krukenberg. 2001. “Motor System: Cortex, Basal Ganglia, and Cerebellum.” <em>Neuroimaging Clinics of North America</em> 11 (2): 203–19.</p>
</div>
<div id="ref-Kaye1980">
<p>Kaye, Jonathan. 1980. “The Mystery of the Tenth Vowel.” <em>Journal of Linguistic Research</em> 1: 1–14.</p>
</div>
<div id="ref-kazanina2006influence">
<p>Kazanina, Nina, Colin Phillips, and William Idsardi. 2006. “The Influence of Meaning on the Perception of Speech Sounds.” <em>Proceedings of the National Academy of Sciences</em> 103 (30). National Acad Sciences: 11381–6.</p>
</div>
<div id="ref-kenstowicz-kisseberth1979">
<p>Kenstowicz, Michael, and Charles Kisseberth. 1979. <em>Generative Phonology: Description and Theory</em>. New York: Academic Press.</p>
</div>
<div id="ref-Kiparsky68">
<p>Kiparsky, Paul. 1968. “How Abstract Is Phonology?” In <em>Three Dimensions of Linguistic Theory</em>, edited by O. Fujimura, 5–56. TEC, Tokyo.</p>
</div>
<div id="ref-Kiparsky1973">
<p>———. 1973. “Abstractness, Opacity and Global Rules.” In <em>Three Dimensions of Linguistic Theory</em>, edited by O. Fujimura, 57–86. Tokyo: TEC.</p>
</div>
<div id="ref-Kiparsky82-cplp">
<p>———. 1982. “From cyclic phonology to lexical phonology.” In <em>The Structure of Phonological Representations, Pt. 1</em>, edited by van der Hulst Harry and Norval Smith, 131–75. Dordrecht: Foris.</p>
</div>
<div id="ref-Kiparsky93">
<p>———. 1993. “Blocking in nonderived environments.” In <em>Studies in Lexical Phonology</em>, edited by Hargus Sharon and Ellen M. Kaisse, 4:277–313. Phonetics and Phonology. San Diego, CA: Academic Press. doi:<a href="https://doi.org/http://dx.doi.org/10.1016/B978-0-12-325071-1.50016-9">http://dx.doi.org/10.1016/B978-0-12-325071-1.50016-9</a>.</p>
</div>
<div id="ref-Kiparsky15">
<p>———. 2015. “Stratal OT: A Synopsis and FAQs.” In <em>Capturing Phonological Shades Within and Across Languages</em>, edited by Yuchau E. Hsiao and Lian-Hee Wee, 2–44. Newcastle upon Tyne: Cambridge Scholars Library.</p>
</div>
<div id="ref-Kisseberth1969">
<p>Kisseberth, Charles W. 1969. “On the abstractness of phonology: The evidence from Yawelmani.” <em>Papers in Linguistics</em> 1 (2): 248–82. doi:<a href="https://doi.org/10.1080/08351816909389119">10.1080/08351816909389119</a>.</p>
</div>
<div id="ref-KlimaBellugi1979">
<p>Klima, Edward S, and Ursula Bellugi. 1979. <em>The Signs of Language</em>. Harvard University Press.</p>
</div>
<div id="ref-Labov2020">
<p>Labov, William. 2020. “The Regularity of Regular Sound Change.” <em>Language</em> 96 (1): 42–59. doi:<a href="https://doi.org/10.1353/lan.2020.0001">10.1353/lan.2020.0001</a>.</p>
</div>
<div id="ref-liddell1984think">
<p>Liddell, Scott K. 1984. “THINK and Believe: Sequentiality in American Sign Language.” <em>Language</em>. JSTOR, 372–99.</p>
</div>
<div id="ref-liddell1989american">
<p>Liddell, Scott K, and Robert E Johnson. 1989. “American Sign Language: The Phonological Base.” <em>Sign Language Studies</em> 64 (1). Gallaudet University Press: 195–277.</p>
</div>
<div id="ref-lillo1997modular">
<p>Lillo-Martin, Diane. 1997. “The Modular Effects of Sign Language Acquisition.” <em>Relations of Language and Thought: The View from Sign Language and Deaf Children</em>, 62–109.</p>
</div>
<div id="ref-lozano1979spanish">
<p>Lozano, María del Carmen. 1979. “Stop and Spirant Alternations: Fortition and Spirantization Processes in Spanish Phonology.” Doctoral dissertation, Columbus, OH: Ohio State University.</p>
</div>
<div id="ref-marsaja2008">
<p>Marsaja, I Gede. 2008. <em>Desa Kolok: A Deaf Village and Its Sign Language in Bali, Indonesia</em>. Ishara Press.</p>
</div>
<div id="ref-martinet1960">
<p>Martinet, André. 1960. “Eléments de Linguistique Générale, Paris, a.” <em>Armand Colin, 1re éd</em>.</p>
</div>
<div id="ref-Mascaro76">
<p>Mascaró, Joan. 1976. “Catalan Phonology and the Phonological Cycle.” Doctoral dissertation, Cambridge, MA: MIT.</p>
</div>
<div id="ref-McCarthy03-dy">
<p>McCarthy, John J. 2003. “Sympathy, cumulativity, and the Duke-of-York gambit.” In <em>The Syllable in Optimality Theory</em>, edited by Féry Caroline and Ruben van de Vijver, 23–76. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-meier2002different">
<p>Meier, Richard P. 2002. “Why Different, Why the Same? Explaining Effects and Non-Effects of Modality Upon Linguistic Structure in Sign and Speech.” <em>Modality and Structure in Signed and Spoken Languages</em>. Cambridge, Cambridge University Press, 1–25.</p>
</div>
<div id="ref-meir2012word">
<p>Meir, Irit. 2012. “Word Classes and Word Formation.” <em>Sign Language. An International Handbook. Berlin: De Gruyter Mounton</em>, 77–112.</p>
</div>
<div id="ref-mesgarani2014phonetic">
<p>Mesgarani, Nima, Connie Cheung, Keith Johnson, and Edward F Chang. 2014. “Phonetic Feature Encoding in Human Superior Temporal Gyrus.” <em>Science</em> 343 (6174). American Association for the Advancement of Science: 1006–10.</p>
</div>
<div id="ref-nevins2011">
<p>Nevins, Andrew. 2011. “Phonologically Conditioned Allomorph Selection.” <em>The Blackwell Companion to Phonology</em>. Wiley Online Library, 1–26.</p>
</div>
<div id="ref-newkirk1981">
<p>Newkirk, Don. 1981. “On the Temporal Segmentation of Movement in Asl.” <em>Unpublished Manuscript, The Salk Institute for Biological Studies</em>.</p>
</div>
<div id="ref-OHara17">
<p>O’Hara, Charlie. 2017. “How abstract is more abstract? Learning abstract underlying representations.” <em>Phonology</em> 34: 325–45.</p>
</div>
<div id="ref-Odden2014">
<p>Odden, David. 2014. <em>Introducing Phonology</em>. 2nd ed. Cambridge University Press.</p>
</div>
<div id="ref-Ohala1981">
<p>Ohala, J.J. 1981. “The Listener as a Source of Sound Change.” In <em>Papers from the Parasession on Language and Behavior: Chicago Linguistics Society</em>, edited by C.S. Masek, R.A. Hendrik, and M.F. Miller, 178–203.</p>
</div>
<div id="ref-Ohala1997">
<p>Ohala, John. 1997. “The Relation Between Phonetics and Phonology.” In <em>Tha Handbook of Phonetic Sciences</em>, edited by William J. Hardcastle and John Laver, 674–94. Blackwell Publishers.</p>
</div>
<div id="ref-ohala1974">
<p>Ohala, Manjari. 1974. “The Abstractness Controversy: Experimental Input from Hindi.” <em>Language</em>. JSTOR, 225–35.</p>
</div>
<div id="ref-Pierrehumbert2002">
<p>Pierrehumbert, Janet B. 2002. “Word-Specific Phonetics.” In <em>Laboratory Phonology 7</em>, edited by C. Gussenhoven and N. Warner. Vol. 4. Phonology and Phonetics 1. Berlin; New York: Mouton de Gruyter.</p>
</div>
<div id="ref-Pierrehumbert2016">
<p>———. 2016. “Phonological Representation: Beyond Abstract Versus Episodic.” <em>Annual Review of Linguistics</em> 2 (1): 33–52.</p>
</div>
<div id="ref-poeppel2012maps">
<p>Poeppel, David. 2012. “The Maps Problem and the Mapping Problem: Two Challenges for a Cognitive Neuroscience of Speech and Language.” <em>Cognitive Neuropsychology</em> 29 (1-2). Taylor &amp; Francis: 34–55.</p>
</div>
<div id="ref-poeppel2011recognizing">
<p>Poeppel, David, and William Idsardi. 2011. “Recognizing Words from Speech: The Perception-Action-Memory Loop.” In <em>Lexical Representation: A Multidisciplinary Approach.</em>, 171–96. Mouton de Gruyter.</p>
</div>
<div id="ref-Pullum76">
<p>Pullum, Geoffrey K. 1976. “The Duke of York gambit.” <em>Journal of Linguistics</em> 12: 83–102.</p>
</div>
<div id="ref-Rawski17">
<p>Rawski, Jonathan. 2017. “Phonological Complexity Across Speech and Sign.” In <em>Proceedings of the 53rd Chicago Linguistics Society Annual Meeting</em>.</p>
</div>
<div id="ref-reiss2018substance">
<p>Reiss, Charles. 2018. “Substance Free Phonology.” <em>The Routledge Handbook of Phonological Theory</em>. Routledge New York, 425–52.</p>
</div>
<div id="ref-Sandler89">
<p>Sandler, Wendy. 1989. <em>Phonological Representation of the Sign: Linearity and Nonlinearity in American Sign Language</em>. Vol. 32. Walter de Gruyter.</p>
</div>
<div id="ref-sandler1993sign">
<p>———. 1993. “Sign Language and Modularity.” <em>Lingua</em> 89 (4). Elsevier: 315–51.</p>
</div>
<div id="ref-sandler1996representing">
<p>———. 1996. <em>Representing Handshapes</em>. Vol. 1. 5. Lawrence Erlbaum Associates, Inc., Mahwah, NJ.</p>
</div>
<div id="ref-sandler2006sign">
<p>Sandler, Wendy, and Diane Lillo-Martin. 2006. <em>Sign Language and Linguistic Universals</em>. Cambridge University Press.</p>
</div>
<div id="ref-sandleretal2005">
<p>Sandler, Wendy, Irit Meir, Carol Padden, and Mark Aronoff. 2005. “The Emergence of Grammar: Systematic Structure in a New Language.” <em>Proceedings of the National Academy of Sciences</em> 102 (7). National Acad Sciences: 2661–5.</p>
</div>
<div id="ref-Sapir1925">
<p>Sapir, Edward. 1925. “Sound Patterns in Language.” <em>Language</em> 1 (2): 37–51.</p>
</div>
<div id="ref-sapir1933">
<p>———. 1933. “La Rèalitè Psychologique Des Phonémes.” <em>Journal de Psychologie Normale et Pathologique</em> 247-65.</p>
</div>
<div id="ref-scharinger2011comp">
<p>Scharinger, Mathias, William J Idsardi, and Samantha Poe. 2011. “A Comprehensive Three-Dimensional Cortical Map of Vowel Space.” <em>Journal of Cognitive Neuroscience</em> 23 (12). MIT Press: 3972–82.</p>
</div>
<div id="ref-scharinger2012asymmetries">
<p>Scharinger, Mathias, Philip J Monahan, and William J Idsardi. 2012. “Asymmetries in the Processing of Vowel Height.” <em>Journal of Speech, Language, and Hearing Research</em>. ASHA.</p>
</div>
<div id="ref-senghasetal2004">
<p>Senghas, Ann, Sotaro Kita, and Asli Özyürek. 2004. “Children Creating Core Properties of Language: Evidence from an Emerging Sign Language in Nicaragua.” <em>Science</em> 305 (5691). American Association for the Advancement of Science: 1779–82.</p>
</div>
<div id="ref-Stokoe1960">
<p>Stokoe, William C. 1960. “Studies in Linguistics: Occasional Papers 8.” In <em>Sign Language Structure: An Outline of the Visual Communication System of the American Deaf</em>. Linstock Press.</p>
</div>
<div id="ref-Strother-Garcia-diss-2019">
<p>Strother-Garcia, Kristina. 2019. “Using Model Theory in Phonology: A Novel Characterization of Syllable Structure and Syllabification.” PhD thesis, University of Delaware.</p>
</div>
<div id="ref-supallanewport1978">
<p>Supalla, Ted, and Liza Newport. 1978. “How Many Seats in a Chair? The Derivation of Nouns and Verbs in American Sign Language, I P. Siple (Ed.): Understanding Language Through Sign Language Research, Academic Press.”</p>
</div>
<div id="ref-Vago76">
<p>Vago, Robert M. 1976. “Theoretical implications of Hungarian vowel harmony.” <em>Linguistic Inquiry</em> 7 (2): 243–63.</p>
</div>
<div id="ref-volenec2017cognitive">
<p>Volenec, Veno, and Charles Reiss. 2017. “Cognitive Phonetics: The Transduction of Distinctive Features at the Phonology–Phonetics Interface.” <em>Biolinguistics</em> 11: 251–94.</p>
</div>
<div id="ref-wilbur2011sign">
<p>Wilbur, Ronnie. 2011. “Sign Syllables.” <em>The Blackwell Companion to Phonology</em> 1: 1309–34.</p>
</div>
<div id="ref-woll2001multilingualism">
<p>Woll, Bencie, Rachel Sutton-Spence, and Frances Elton. 2001. “Multilingualism: The Global Approach to Sign Languages.” <em>The Sociolinguistics of Sign Languages</em>. Cambridge University Press Cambridge, UK, 8–32.</p>
</div>
<div id="ref-wootton2015invention">
<p>Wootton, David. 2015. <em>The Invention of Science: A New History of the Scientific Revolution</em>. Penguin UK.</p>
</div>
<div id="ref-Yip2002">
<p>Yip, Moira. 2002. <em>Tone</em>. Cambridge University Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>One of the clearest recent expositions on abstractness and its virtues is in Ch. 2 of <span class="citation">Cheng (2015)</span>’s book, <em>How to Bake <span class="math inline"><em>π</em></span></em>, which we encourage readers of the present chapter to read.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Here and elsewhere in this chapter we follow traditional phonological notation of transcribing the mental representation of speech between slashes and the actual pronunciaton within square brackets. When the distinction is immaterial, we use italics.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>More — and more detailed — arguments against this ‘morpheme alternant theory’ can be found in <span class="citation">Kenstowicz and Kisseberth (1979, 180–96)</span>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>The ‘+’ indicates the presumed morphological boundary between the 1sg habitual prefix and the verb stem .<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>A much more comprehensive historical context can be gotten from <span class="citation">Anderson (1985)</span>; see in particular pp. 270–276.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p><span class="citation">Bloch (1941, 281–83)</span> observed that this represents a potential learning challenge, because arriving at the ‘right’ underlying form for a morpheme requires exposure to a sufficient variety of its surface manifestations.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>Narrower differences include the precise tongue tip position for the articulation of (alveolar in English, dental in Spanish) and the degree of constriction of (more close in English, more open in Spanish).<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Whether or is the right representation of this phoneme in Spanish is a matter of some debate. <span class="citation">Harris (1969)</span> says , <span class="citation">Baković (1994)</span> says , while <span class="citation">Lozano (1979)</span> opts for underspecification of the difference between the two speech sounds.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>Or very similar, in some respects, by way of a non-spurious there-and-back-again sequence of transformations known as a <em>Duke of York derivation</em> (see <span class="citation">Pullum (1976; McCarthy 2003; Baković 2013)</span> for discussion).<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>We write ‘must not apply before’ as opposed to ‘must apply after’ because these opaque interactions can be had with simultaneous as opposed to ordered application of phonological transformations <span class="citation">(Kenstowicz and Kisseberth 1979; Joshi and Kiparsky 1979; Joshi and Kiparsky 2006; Kiparsky 2015)</span>. See also <span class="citation">Baković and Blumenfeld (2017)</span> for discussion.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>We put aside the precise formulation of <span class="citation">Kiparsky (1968)</span>’s principle, as well as of its ensuing revisions <span class="citation">(Mascaró 1976; Kiparsky 1982; Kiparsky 1993)</span>, focusing instead on its intended function.<a href="#fnref11">↩</a></p></li>
</ol>
</div>
</body>
</html>
