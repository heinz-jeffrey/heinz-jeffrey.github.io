% Learnability (LIN 629)

# Course Information

**Course:** TuTh 09:30-10:50

**Instructor:** [Jeffrey Heinz](http://jeffreyheinz.net/), [jeffrey.heinz@stonybrook.edu](mailto:jeffrey.heinz@stonybrook.edu)

**Office Hours:** WF 3:30pm-5:00pm

# Materials

* [Syllabus](materials/learnability-LIN629-25F-Heinz.pdf)

# Course Log

## 13 Sep 2025

* We discussed some of your ideas on how to modify identification in the limit.
* We discussed language learning on [planet
  Verorez](materials/04verorez.pdf).
* Read [Heinz 2010](https://aclanthology.org/P10-1092/) for Thursday
  (Abed will present it within 40 minutes).

## 11 Sep 2025

* We went over the proof that the one can identify the class of
  TM-computable languages from texts that generated by primitive
  recursive functions (using the generator naming relation as opposed
  to the tester).
* The basic idea behind the proof is that the text itself in a
  generator/enumerator of the language and so there is a p.r. program
  for it. The learner simply guesses the first program in the
  enumeration of all programs that correctly predicts the text it has
  seen so far. After finitely many guesses, it is guaranteed to find
  the first program generating the text, and thus have found a program
  generating the target language.
* We then began discussing your reflections on Gold. Here is a list of
  the issues the class identified.
  1. The paradigm targets formal languages, which is about weak
     generative capacity and not strong generative capacity.
  2. The paradigm considers data presentations without noise. But real
     world data is noisy with false positives, false negatives, or
     missing data.
  3. The paradigm does not consider time/resource complexity. While it
     may be useful to say "no algorithm exists no matter how many
     resources are consumed" we also want to be interested in learners
     that make efficient use of the data.
  4. The paradigm requires exact identification, which is too strict a
     requirement because may learn something close enough to the
     target language, even if it is not exactly the same. (this can
     also help explain why languages change)
  5. The learner does not know when convergence happens.
  6. As a model of acquisition, the paradigm fails to take into
     account the critical period. Humans succeed before this window
     closes.
  7. As a model of acquisition, the paradigm takes into account
     adversarial data presentations; that is, ones that potentially
     delay key data points for as long as possible. Human children are
     probably not exposed to adversarial data presentations.
  8. As a model of acquisition, how would the model take into account
     U-shaped learning, where children, correctly generalize,
     overgeneralize, and then correctly generalize again.
  9. The paradigm does not appear to make distinctions between
     learning grammars, searching for grammars in a space of grammars,
     and memorizing strings.
* Your HW for Tuesday, is the following: Pick one of the above issues,
  and provide 1-2 ideas about how the id in the limit paradigm could
  be modified to address the issue. Again, please limit your response
  to 1-2 paragraphs (max one page). You do not have to provide a
  formal definition of learning, but you are welcome to. You are also
  welcome to reflect whether your proposed solution allows edge cases
  that makes instances of the learning problem "unreasonably"
  difficult (for instance if you are trying to learn in the presence
  of noise, how do you ensure the noise doesn't drown out the signal?)

## 09 Sep 2025

* We reviewed the proofs of theorems 4 and 6 and discussed them.
* We proved the following statement: Any finite class of languages is
  identifiable in the limit (hypothesizing testers) from positive
  (arbitrary) data.
* The idea behind this proof is that any finite class of languages can
  be ordered in such a way such that, for any pair of languages (L,
  L') in a proper subset relationship (so L is a subset of L'), (a
  grammar for) language L is ordered before (a grammar for) language
  L'. Then a learning strategy is to find the first grammar in the
  list consistent with the data so far. 
* Homework for Thursday: Write 1-2 paragraphs (no more than one page),
  reflecting on the identification in the limit paradigm. What are
  some choices in its definition of learning that you would reconsider
  and why? Please email the assignment to me.

<!-- * Time permitting, we considered the learnability of languages spoken -->
<!--   on the planet [Verorez](materials/04verorez.pdf). -->
<!-- * For Thursday, begin reading [chapter 3](materials/HdlHvZ2015.pdf) on -->
<!--   state-merging. -->


## 04 Sep 2025

* We went through the notes on [id in the
  limit](materials/03idlimit.pdf). In particular:
* We reviewed identification in the limit of testers from arbitrary
  text (commonly known as identification in the limit from positive
  data).
* We examined some two classes of formal languages (Strictly
  k-Piecewise and Strictly k-Local) and algorithms which can learn
  these classes under this definition of learning. Both exemplify
  "string extension learning" which is a particular kind of learning
  strategy. It has other properties as well which we will return to later.
* At the end of class we considered another strategy based on
  enumeration, and presented an algorithm which learns the class BAR-X
  under this definition.
* For next Tuesday please study Theorems 4 and 6 and their
  proofs. (You can skip Theorems 5 and 7 for now.)

## 02 Sep 2025

* We discussed the twelve identification in the limit paradigms that
  Gold introduced, the main results, and Gold's discussion of the
  implications of these results vis a vis child acquisition.
* We took a brief aside on [recursive function
  theory](materials/recursive-function-theory.pdf) to get an idea of
  some of the finer points of arbitrary functions/sets/text versus
  recursive functions/sets/text versus primitive recursive
  functions/sets/text.
*  Here are some notes on [id in the
  limit](materials/03idlimit.pdf). Please read up to, but not
  including, section 2.4 for Thursday. Document the mistakes!

## 28 Aug 2025

* We discussed finished talking about likelihood.
* We continued talking about learning as a computational problem.
* We discussed the first few sections of Gold 1967.
* We are prepared to discuss [enumeration](materials/02enumeration.pdf),
  which plays an important role in computability.
* For next Tuesday, read section 4 of Gold 1967.

## 26 Aug 2025

* We went over the syllabus.
* We introduced the idea of studying learning as a computational problem.
* We studied the meanings of consistent estimators.
* [01intro.pdf](materials/01intro.pdf)
* For Thursday: please read sections 1-3 of [Gold
  1967](materials/Gold--1967--LanguageIdentificationInTheLimit.pdf).

-------------------------------------------------------------------------------
