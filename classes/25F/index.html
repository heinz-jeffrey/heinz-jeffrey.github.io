<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Learnability (LIN 629)</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="my.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Learnability (LIN 629)</h1>
</header>
<h1 id="course-information">Course Information</h1>
<p><strong>Course:</strong> TuTh 09:30-10:50</p>
<p><strong>Instructor:</strong> <a href="http://jeffreyheinz.net/">Jeffrey Heinz</a>, <a href="mailto:jeffrey.heinz@stonybrook.edu">jeffrey.heinz@stonybrook.edu</a></p>
<p><strong>Office Hours:</strong> WF 3:30pm-5:00pm</p>
<h1 id="materials">Materials</h1>
<ul>
<li><a href="materials/learnability-LIN629-25F-Heinz.pdf">Syllabus</a></li>
</ul>
<h1 id="course-log">Course Log</h1>
<h2 id="upcoming">Upcoming</h2>
<p>(subject to change)</p>
<ul>
<li>Thu 10/16 - <a href="https://aclanthology.org/W19-5708/">BUFIA</a> by Ola</li>
<li>Tue 10/21 - <a href="https://www.pnas.org/doi/full/10.1073/pnas.2021865119">Yang and Piantadosi 2022</a> by Robin</li>
<li>Thu 10/23 - One of <a href="materials/delaHiguera2010.pdf">OSTIA (see chapter 18)</a>, <a href="https://transacl.org/ojs/index.php/tacl/article/view/429/84">ISLFLA</a>, or <a href="https://proceedings.mlr.press/v34/jardine14a.html">SOSFIA</a> (DF Transducer learning) by Dorothy</li>
<li>Tue 10/28 - <a href="https://proceedings.mlr.press/v153/hua21a.html">UR learning</a> by Ben</li>
<li>Thu 10/30 - <a href="http://jeffreyheinz.net/papers/Heinz-2016-CTLDP.pdf">review paper</a> by Geonhee</li>
<li>Tue 11/04 - something related to syntactic learning (precise paper TBD) by Alina</li>
</ul>
<h2 id="oct-2025">09 Oct 2025</h2>
<ul>
<li>We made plans for papers and topics to study in upcoming weeks.</li>
<li>We discussed in broad terms ideas for final projects.
<ul>
<li>theoretical work – identify linguistically motivated classes of concepts learnable under some criterion</li>
<li>theoretical/empirical work – identify weakness in some current learning algorithm and improve it</li>
<li>empirical work – compare a new variant of a learning algorithm (that you are introducing) to the existing version on one or more datasets</li>
<li>empirical work – compare an existing learning algorithm on one or more datasets to others (such as NNs or maxent)</li>
</ul></li>
<li>We began the formal <a href="materials/05pacintro.pdf">introduction to PAC</a></li>
</ul>
<h2 id="oct-2025-1">07 Oct 2025</h2>
<ul>
<li>Jacob presented Angluin’s proof, reading <a href="materials/Angluin1980-iiflpd.pdf">Angluin 1980</a>, section 3q, condition 1 and/or 2 (TBD)</li>
<li>We introduced the axis-aligned rectangle learning problem and identified the strategy of selecting the smallest axis-aligned rectangle which fits the positive data will as one that will succeed if all positive data points are eventually observed.</li>
</ul>
<h2 id="oct-2025-2">02 Oct 2025</h2>
<ul>
<li>M presented <a href="https://www.jmlr.org/papers/volume8/clark07a/clark07a.pdf">Clark and Eyraud 2007</a>,</li>
<li>We continued with PAC, getting through section 5.6 of <a href="materials/Valiant2013.pdf">Valiant 2013</a>.</li>
</ul>
<h2 id="sep-2025">30 Sep 2025</h2>
<ul>
<li>Matthew presented <a href="https://www.jeffreyheinz.net/papers/Heinz-2009-RLLSP.pdf">Heinz 2009</a>, on learning stress patterns with neighborhood distinctness.</li>
<li>We began our introduction to PAC. We got through sections 5.2 and 5.3 of <a href="materials/Valiant2013.pdf">Valiant 2013</a>.</li>
<li>PAC readings in the near term which we will be going over in class.
<ul>
<li>Sections 5.2 - 5.6 (inclusive) of <a href="materials/Valiant2013.pdf">Valiant 2013</a>.</li>
<li>And then up to 1.4 of <a href="materials/KV94.pdf">Kearns and Vazirani 1994</a></li>
</ul></li>
</ul>
<h2 id="sep-2025-1">25 Sep 2025</h2>
<ul>
<li>Hannah presented on <a href="https://proceedings.mlr.press/v217/wu23a/wu23a.pdf">Wu and Heinz 2023</a>. We also discussed the limitations of using relative frequencies as a way to distinguish between well-formed and ill-formed expressions.</li>
<li>We discussed an efficient, state-merging method “RPNI”, which identifies the class of regular languages in the limit from positive and negative data. See <a href="materials/delaHiguera2010.pdf">de la Higuera 2010</a> for more details (Chapter 12).</li>
</ul>
<h2 id="sep-2025-2">23 Sep 2025</h2>
<ul>
<li>We discussed some items in the string extension learning paper.
<ul>
<li>the characteristic sample</li>
<li>the example relating to the Parikh map</li>
<li>the example relating to the generalized subsequence languages</li>
</ul></li>
<li>We discussed Angluin’s theorem, but did not prove it. <strong>Claimed by Jacob to show proof on 10/07!</strong></li>
<li>We reviewed three ways to learn k-SL languages
<ol type="1">
<li>collect permissible k-factors</li>
<li>build prefix tree and merge states with a k-1 prefix</li>
<li>“filling in” the transitions and states of a k-definite machine</li>
</ol></li>
<li>We observed that each method is an instantiation of a more general strategy
<ol type="1">
<li>string extension learning</li>
<li>state-merging</li>
<li>activating parts of a fixed, deterministic grammar</li>
</ol></li>
<li>We observed that string extension learning has been generalized to lattice learning <a href="https://www.jeffreyheinz.net/papers/Heinz-KasprzikEtAl-2012-LLHS.pdf">(Heinz, Kasprzik, and Kötzing 2013)</a></li>
<li>We observed that the “filling in” method
<ul>
<li>can be generalized to multiple finite DFA (for handling the k-SP language) <a href="http://jeffreyheinz.net/papers/Heinz-Rogers-2013-LSCLFDA.pdf">(Heinz and Rogers 2013)</a></li>
<li>has a probabilistic variant, which is the classical n-gram model, which finds the MLE of the data with respect to this type of model.</li>
<li>(and the probabilistic variant can also be extended to multiple DFA <a href="http://jeffreyheinz.net/papers/Shibata-Heinz-2019-MLEFRDSL.pdf">(Shibata and Heinz 2018)</a> )</li>
</ul></li>
<li>Resources for learning more on state merging
<ul>
<li><a href="materials/delaHiguera2010.pdf">de la Higuera 2010</a> is a comprehensive, technical treatment</li>
<li><a href="materials/HdlHvZ2015.pdf">Heinz, de la Higuera, and van Zaanen 2015</a> is less technical, and covers main ideas</li>
</ul></li>
<li>For Thursday:</li>
</ul>
<h2 id="sep-2025-3">18 Sep 2025</h2>
<ul>
<li>We reviewed the zero-reversible languages and the state-merging algorithm for learning them.</li>
<li>We discussed state-merging strategies more broadly for learning regular languages.</li>
<li>Abed presented on string extension learning.</li>
<li>Future topics for possible presentation
<ul>
<li>a state-merging strategy for learning stress patterns <a href="https://www.jeffreyheinz.net/papers/Heinz-2009-RLLSP.pdf">Heinz 2009</a> <strong>Claimed by Matthew for 9/30!</strong></li>
<li>RPNI - efficient learning of the class of regular languages from positive and negative data <a href="materials/delaHiguera2010.pdf">de la Higuera 2010, Chapter 12 section 4</a></li>
<li>Substitutable languages (a subclass of CF languages) <a href="https://www.jmlr.org/papers/volume8/clark07a/clark07a.pdf">Clark and Eyraud 2007</a> <strong>Claimed by M for 10/02!</strong></li>
<li>Sting extension learning with noise <a href="https://proceedings.mlr.press/v217/wu23a/wu23a.pdf">Wu and Heinz 2023</a> <strong>Claimed by Hannah for 9/25!</strong> <!-- - Distributional learning
  more generally []() --> <!-- - Efficiency Concerns [Eyraud et
  al. 2014]() --></li>
</ul></li>
</ul>
<h2 id="sep-2025-4">16 Sep 2025</h2>
<ul>
<li>We discussed some of your ideas on how to modify identification in the limit.</li>
<li>We discussed language learning on <a href="materials/04verorez.pdf">planet Verorez</a>.</li>
<li>Read <a href="https://aclanthology.org/P10-1092/">Heinz 2010</a> for Thursday (Abed will present it within 40 minutes).</li>
</ul>
<h2 id="sep-2025-5">11 Sep 2025</h2>
<ul>
<li>We went over the proof that the one can identify the class of TM-computable languages from texts that generated by primitive recursive functions (using the generator naming relation as opposed to the tester).</li>
<li>The basic idea behind the proof is that the text itself in a generator/enumerator of the language and so there is a p.r. program for it. The learner simply guesses the first program in the enumeration of all programs that correctly predicts the text it has seen so far. After finitely many guesses, it is guaranteed to find the first program generating the text, and thus have found a program generating the target language.</li>
<li>We then began discussing your reflections on Gold. Here is a list of the issues the class identified.
<ol type="1">
<li>The paradigm targets formal languages, which is about weak generative capacity and not strong generative capacity.</li>
<li>The paradigm considers data presentations without noise. But real world data is noisy with false positives, false negatives, or missing data.</li>
<li>The paradigm does not consider time/resource complexity. While it may be useful to say “no algorithm exists no matter how many resources are consumed” we also want to be interested in learners that make efficient use of the data.</li>
<li>The paradigm requires exact identification, which is too strict a requirement because may learn something close enough to the target language, even if it is not exactly the same. (this can also help explain why languages change)</li>
<li>The learner does not know when convergence happens.</li>
<li>As a model of acquisition, the paradigm fails to take into account the critical period. Humans succeed before this window closes.</li>
<li>As a model of acquisition, the paradigm takes into account adversarial data presentations; that is, ones that potentially delay key data points for as long as possible. Human children are probably not exposed to adversarial data presentations.</li>
<li>As a model of acquisition, how would the model take into account U-shaped learning, where children, correctly generalize, overgeneralize, and then correctly generalize again.</li>
<li>The paradigm does not appear to make distinctions between learning grammars, searching for grammars in a space of grammars, and memorizing strings.</li>
</ol></li>
<li>Your HW for Tuesday, is the following: Pick one of the above issues, and provide 1-2 ideas about how the id in the limit paradigm could be modified to address the issue. Again, please limit your response to 1-2 paragraphs (max one page). You do not have to provide a formal definition of learning, but you are welcome to. You are also welcome to reflect whether your proposed solution allows edge cases that makes instances of the learning problem “unreasonably” difficult (for instance if you are trying to learn in the presence of noise, how do you ensure the noise doesn’t drown out the signal?)</li>
</ul>
<h2 id="sep-2025-6">09 Sep 2025</h2>
<ul>
<li>We reviewed the proofs of theorems 4 and 6 and discussed them.</li>
<li>We proved the following statement: Any finite class of languages is identifiable in the limit (hypothesizing testers) from positive (arbitrary) data.</li>
<li>The idea behind this proof is that any finite class of languages can be ordered in such a way such that, for any pair of languages (L, L’) in a proper subset relationship (so L is a subset of L’), (a grammar for) language L is ordered before (a grammar for) language L’. Then a learning strategy is to find the first grammar in the list consistent with the data so far.</li>
<li>Homework for Thursday: Write 1-2 paragraphs (no more than one page), reflecting on the identification in the limit paradigm. What are some choices in its definition of learning that you would reconsider and why? Please email the assignment to me.</li>
</ul>
<!-- * Time permitting, we considered the learnability of languages spoken -->
<!--   on the planet [Verorez](materials/04verorez.pdf). -->
<!-- * For Thursday, begin reading [chapter 3](materials/HdlHvZ2015.pdf) on -->
<!--   state-merging. -->
<h2 id="sep-2025-7">04 Sep 2025</h2>
<ul>
<li>We went through the notes on <a href="materials/03idlimit.pdf">id in the limit</a>. In particular:</li>
<li>We reviewed identification in the limit of testers from arbitrary text (commonly known as identification in the limit from positive data).</li>
<li>We examined some two classes of formal languages (Strictly k-Piecewise and Strictly k-Local) and algorithms which can learn these classes under this definition of learning. Both exemplify “string extension learning” which is a particular kind of learning strategy. It has other properties as well which we will return to later.</li>
<li>At the end of class we considered another strategy based on enumeration, and presented an algorithm which learns the class BAR-X under this definition.</li>
<li>For next Tuesday please study Theorems 4 and 6 and their proofs. (You can skip Theorems 5 and 7 for now.)</li>
</ul>
<h2 id="sep-2025-8">02 Sep 2025</h2>
<ul>
<li>We discussed the twelve identification in the limit paradigms that Gold introduced, the main results, and Gold’s discussion of the implications of these results vis a vis child acquisition.</li>
<li>We took a brief aside on <a href="materials/recursive-function-theory.pdf">recursive function theory</a> to get an idea of some of the finer points of arbitrary functions/sets/text versus recursive functions/sets/text versus primitive recursive functions/sets/text.</li>
<li>Here are some notes on <a href="materials/03idlimit.pdf">id in the limit</a>. Please read up to, but not including, section 2.4 for Thursday. Document the mistakes!</li>
</ul>
<h2 id="aug-2025">28 Aug 2025</h2>
<ul>
<li>We discussed finished talking about likelihood.</li>
<li>We continued talking about learning as a computational problem.</li>
<li>We discussed the first few sections of Gold 1967.</li>
<li>We are prepared to discuss <a href="materials/02enumeration.pdf">enumeration</a>, which plays an important role in computability.</li>
<li>For next Tuesday, read section 4 of Gold 1967.</li>
</ul>
<h2 id="aug-2025-1">26 Aug 2025</h2>
<ul>
<li>We went over the syllabus.</li>
<li>We introduced the idea of studying learning as a computational problem.</li>
<li>We studied the meanings of consistent estimators.</li>
<li><a href="materials/01intro.pdf">01intro.pdf</a></li>
<li>For Thursday: please read sections 1-3 of <a href="materials/Gold--1967--LanguageIdentificationInTheLimit.pdf">Gold 1967</a>.</li>
</ul>
<hr />

<p><i><script language="Javascript">
document.write("Last updated: " + document.lastModified +"");
</SCRIPT></i></p>
</body>
</html>
